{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Check the current version of Keras\n",
        "!pip show tensorflow\n",
        "!pip show keras\n",
        "\n",
        "# Uninstall the current version of Keras\n",
        "!pip uninstall tensorflow -y\n",
        "!pip uninstall keras -y\n",
        "\n",
        "# Install the specific version of Keras\n",
        "!pip install tensorflow==2.15.1\n",
        "# !pip install keras==2.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eqa05Il1Jvka",
        "outputId": "f87029f4-bb82-4569-eecd-7c0f208dc721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.17.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine_rl, tf_keras\n",
            "Name: keras\n",
            "Version: 3.4.1\n",
            "Summary: Multi-backend Keras.\n",
            "Home-page: https://github.com/keras-team/keras\n",
            "Author: Keras team\n",
            "Author-email: keras-users@googlegroups.com\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, h5py, ml-dtypes, namex, numpy, optree, packaging, rich\n",
            "Required-by: tensorflow\n",
            "Found existing installation: tensorflow 2.17.0\n",
            "Uninstalling tensorflow-2.17.0:\n",
            "  Successfully uninstalled tensorflow-2.17.0\n",
            "Found existing installation: keras 3.4.1\n",
            "Uninstalling keras-3.4.1:\n",
            "  Successfully uninstalled keras-3.4.1\n",
            "Collecting tensorflow==2.15.1\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.15.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.1)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.1)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n",
            "Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.3.2 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "print(keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fheb4tU-J-yW",
        "outputId": "1a9d29f2-ea9e-4190-d3ff-e88780cb766e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ3Ug8GeMyuj",
        "outputId": "65bb9a47-0082-4497-8123-9795b28c6d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO29vhuLutlU"
      },
      "outputs": [],
      "source": [
        "# !pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRZQhWzRwicB"
      },
      "outputs": [],
      "source": [
        "# !pip install np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGik8D2HMOEv",
        "outputId": "80d98564-c854-4a78-b3d4-1a21cde7f86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2KyiZyru_Mm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras import utils\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import load_model, model_from_json\n",
        "\n",
        "import optuna\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOPW21GD0-JE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae6b9b4-44a5-482c-c60a-9c494d5c808f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 3s 0us/step\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n",
            "CIFAR-10 dataset loaded\n",
            "Build and save ResNet-50 model\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 523s 198ms/step - loss: 0.7219 - acc: 0.3957 - val_loss: 0.5019 - val_acc: 0.7777\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 497s 199ms/step - loss: 0.5669 - acc: 0.6333 - val_loss: 0.3846 - val_acc: 0.8660\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 496s 198ms/step - loss: 0.4415 - acc: 0.7425 - val_loss: 0.2719 - val_acc: 0.8994\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 496s 198ms/step - loss: 0.3231 - acc: 0.8027 - val_loss: 0.1759 - val_acc: 0.9172\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 496s 199ms/step - loss: 0.2287 - acc: 0.8415 - val_loss: 0.1132 - val_acc: 0.9277\n"
          ]
        }
      ],
      "source": [
        "# dipakai\n",
        "conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "train_size = int(1 * x_train.shape[0])\n",
        "\n",
        "indices = np.random.permutation(x_train.shape[0])\n",
        "x_train = x_train[indices][:train_size]\n",
        "y_train = y_train[indices][:train_size]\n",
        "\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# x_train = x_train[:int(.30*len(idx))]\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "print('CIFAR-10 dataset loaded')\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.UpSampling2D((2,2),input_shape=(32, 32, 3)))\n",
        "model.add(layers.UpSampling2D((2,2)))\n",
        "model.add(layers.Resizing(200, 200))\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "\toptimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "\tloss='binary_crossentropy',\n",
        "\tmetrics=['acc'])\n",
        "\n",
        "print('Build and save ResNet-50 model')\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "\tepochs=5,\n",
        "\tbatch_size=20,\n",
        "\tvalidation_data=(x_test, y_test))\n",
        "\n",
        "# model_json = model.to_json()\n",
        "# with open('model_resnet.json', 'w') as json_file:\n",
        "# \tjson_file.write(model_json)\n",
        "# model.save_weights('model_resnet.h5')\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open('model_resnet.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Menyimpan bobot model ke file HDF5\n",
        "model.save_weights('model_resnet.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sMhuUwpqFx6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Memuat dataset CIFAR-10\n",
        "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# train_size = int(1 * x_train.shape[0])\n",
        "# indices = np.random.permutation(x_train.shape[0])\n",
        "# x_train = x_train[indices][:train_size]\n",
        "# y_train = y_train[indices][:train_size]\n",
        "\n",
        "# x_train = x_train / 255.0\n",
        "# x_test = x_test / 255.0\n",
        "\n",
        "# y_train = to_categorical(y_train, 10)\n",
        "# y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# print('CIFAR-10 dataset loaded')\n",
        "\n",
        "# conv_base = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
        "# model = models.Sequential()\n",
        "# model.add(layers.UpSampling2D((2,2), input_shape=(32, 32, 3)))\n",
        "# model.add(layers.UpSampling2D((2,2)))\n",
        "# model.add(layers.Resizing(200, 200))\n",
        "# model.add(conv_base)\n",
        "# model.add(layers.Flatten(name='flatten'))  # Menambahkan nama ke layer Flatten\n",
        "# model.add(layers.BatchNormalization())\n",
        "# model.add(layers.Dense(128, activation='relu'))\n",
        "# model.add(layers.Dropout(0.5))\n",
        "# model.add(layers.BatchNormalization())\n",
        "# model.add(layers.Dense(64, activation='relu'))\n",
        "# model.add(layers.Dropout(0.5))\n",
        "# model.add(layers.BatchNormalization())\n",
        "# model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "#     loss='binary_crossentropy',\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# print('Build and save ResNet-50 model')\n",
        "\n",
        "# history = model.fit(x_train, y_train,\n",
        "#     epochs=5,\n",
        "#     batch_size=20,\n",
        "#     validation_data=(x_test, y_test))\n",
        "\n",
        "# # Menyimpan arsitektur model ke file JSON\n",
        "# model_json = model.to_json()\n",
        "# with open('model_resnet.json', 'w') as json_file:\n",
        "#     json_file.write(model_json)\n",
        "\n",
        "# # Menyimpan bobot model ke file HDF5 dengan ekstensi yang benar\n",
        "# model.save_weights('model_resnet.weights.h5')"
      ],
      "metadata": {
        "id": "N9smJ56eWCRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_json = model.to_json()\n",
        "# with open('model_resnet.json', 'w') as json_file:\n",
        "# \tjson_file.write(model_json)\n",
        "# model.save_weights('model_resnet.h5')"
      ],
      "metadata": {
        "id": "5XPU1D2yeUUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1F3iacUHT5a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSgfkun7mgs1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90468abc-237e-452a-86a6-fb3e4bfd79dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 28s 81ms/step\n",
            "Accuracy: 0.9277\n",
            "Precision: 0.9282809858206612\n",
            "Recall: 0.9277000000000001\n",
            "F1 Score: 0.927727312168571\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
        "precision = precision_score(y_test_classes, y_pred_classes, average='macro')\n",
        "recall = recall_score(y_test_classes, y_pred_classes, average='macro')\n",
        "f1 = f1_score(y_test_classes, y_pred_classes, average='macro')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dipakai\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "def load_cnn_model(X_test, y_test):\n",
        "    json_file = open('model_resnet.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    model = model_from_json(loaded_model_json)\n",
        "    model.load_weights('model_resnet.h5')  # load weights into new model\n",
        "\n",
        "    # Use categorical_crossentropy since it's multi-class classification\n",
        "    model.compile(\n",
        "        optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "        loss='categorical_crossentropy',  # Corrected from binary_crossentropy\n",
        "        metrics=['accuracy']  # 'acc' is deprecated\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_feature_layer(model, data):\n",
        "    total_layers = len(model.layers)\n",
        "    fl_index = total_layers - 7  # Get the correct index for the feature layer\n",
        "\n",
        "    feature_layer_model = keras.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=model.get_layer(index=fl_index).output\n",
        "    )\n",
        "    feature_layer_output = feature_layer_model.predict(data)\n",
        "\n",
        "    return feature_layer_output\n",
        "\n",
        "def xgb_model(X_train, y_train, X_test, y_test):\n",
        "    # Ensure labels are in 1D and consistent with the number of samples\n",
        "    y_train = y_train.flatten()  # Converts to 1D if not already\n",
        "    y_test = y_test.flatten()\n",
        "\n",
        "    dtrain = xgb.DMatrix(\n",
        "        X_train,\n",
        "        label=y_train  # Corrected, was y_test before\n",
        "    )\n",
        "\n",
        "    dtest = xgb.DMatrix(\n",
        "        X_test,\n",
        "        label=y_test  # Corrected, was y_train before\n",
        "    )\n",
        "\n",
        "    params = {\n",
        "        'max_depth': 12,\n",
        "        'eta': 0.05,\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': 10,\n",
        "        'early_stopping_rounds': 5,\n",
        "        'eval_metric': 'merror'\n",
        "    }\n",
        "\n",
        "    watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
        "    n_rounds = 150\n",
        "\n",
        "    model = xgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        n_rounds,\n",
        "        evals=[(dtrain, 'train'), (dtest, 'eval')]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "    y_pred_prob = model.predict(dtest)\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'F1 Score: {f1}')\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def main():\n",
        "    # Load CIFAR-10 dataset\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    indices = np.random.permutation(x_train.shape[0])\n",
        "    X_train = X_train[indices][:train_size]\n",
        "    y_train = y_train[indices][:train_size]\n",
        "\n",
        "    # Normalize the data\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "    # Convert labels to single-dimensional integers\n",
        "    y_train = y_train.flatten()  # Ensure labels are flattened to 1D\n",
        "    y_test = y_test.flatten()\n",
        "\n",
        "    # Load the CNN model and extract features\n",
        "    cnn_model = load_cnn_model(X_test, y_test)\n",
        "\n",
        "    X_train_cnn = get_feature_layer(cnn_model, X_train)  # Extract features\n",
        "    X_test_cnn = get_feature_layer(cnn_model, X_test)  # Extract features\n",
        "\n",
        "    # Train XGBoost model with correct objective function and label format\n",
        "    model = xgb_model(X_train_cnn, y_train, X_test_cnn, y_test)\n",
        "\n",
        "    print(\"Build and save of XGBoost Model\")\n",
        "    pickle.dump(model, open(\"cnn_xgboost_resnet_final.pickle.dat\", \"wb\"))\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model, X_test_cnn, y_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "hjYb8uwAfBZe",
        "outputId": "96d23ebe-1f7a-4fc7-8fd8-32c68e1f28a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 120s 76ms/step\n",
            "313/313 [==============================] - 25s 76ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [05:29:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"early_stopping_rounds\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-merror:0.01666\teval-merror:0.08030\n",
            "[1]\ttrain-merror:0.01538\teval-merror:0.07780\n",
            "[2]\ttrain-merror:0.01478\teval-merror:0.07630\n",
            "[3]\ttrain-merror:0.01414\teval-merror:0.07560\n",
            "[4]\ttrain-merror:0.01344\teval-merror:0.07580\n",
            "[5]\ttrain-merror:0.01318\teval-merror:0.07530\n",
            "[6]\ttrain-merror:0.01296\teval-merror:0.07490\n",
            "[7]\ttrain-merror:0.01270\teval-merror:0.07400\n",
            "[8]\ttrain-merror:0.01248\teval-merror:0.07350\n",
            "[9]\ttrain-merror:0.01242\teval-merror:0.07350\n",
            "[10]\ttrain-merror:0.01228\teval-merror:0.07380\n",
            "[11]\ttrain-merror:0.01208\teval-merror:0.07370\n",
            "[12]\ttrain-merror:0.01176\teval-merror:0.07330\n",
            "[13]\ttrain-merror:0.01154\teval-merror:0.07390\n",
            "[14]\ttrain-merror:0.01148\teval-merror:0.07420\n",
            "[15]\ttrain-merror:0.01126\teval-merror:0.07320\n",
            "[16]\ttrain-merror:0.01098\teval-merror:0.07330\n",
            "[17]\ttrain-merror:0.01086\teval-merror:0.07300\n",
            "[18]\ttrain-merror:0.01072\teval-merror:0.07300\n",
            "[19]\ttrain-merror:0.01044\teval-merror:0.07260\n",
            "[20]\ttrain-merror:0.01040\teval-merror:0.07270\n",
            "[21]\ttrain-merror:0.01022\teval-merror:0.07270\n",
            "[22]\ttrain-merror:0.01012\teval-merror:0.07260\n",
            "[23]\ttrain-merror:0.00998\teval-merror:0.07280\n",
            "[24]\ttrain-merror:0.00976\teval-merror:0.07300\n",
            "[25]\ttrain-merror:0.00954\teval-merror:0.07330\n",
            "[26]\ttrain-merror:0.00930\teval-merror:0.07330\n",
            "[27]\ttrain-merror:0.00904\teval-merror:0.07340\n",
            "[28]\ttrain-merror:0.00880\teval-merror:0.07320\n",
            "[29]\ttrain-merror:0.00866\teval-merror:0.07340\n",
            "[30]\ttrain-merror:0.00848\teval-merror:0.07300\n",
            "[31]\ttrain-merror:0.00826\teval-merror:0.07300\n",
            "[32]\ttrain-merror:0.00810\teval-merror:0.07330\n",
            "[33]\ttrain-merror:0.00780\teval-merror:0.07320\n",
            "[34]\ttrain-merror:0.00756\teval-merror:0.07360\n",
            "[35]\ttrain-merror:0.00738\teval-merror:0.07330\n",
            "[36]\ttrain-merror:0.00708\teval-merror:0.07310\n",
            "[37]\ttrain-merror:0.00686\teval-merror:0.07350\n",
            "[38]\ttrain-merror:0.00666\teval-merror:0.07340\n",
            "[39]\ttrain-merror:0.00656\teval-merror:0.07320\n",
            "[40]\ttrain-merror:0.00642\teval-merror:0.07320\n",
            "[41]\ttrain-merror:0.00630\teval-merror:0.07320\n",
            "[42]\ttrain-merror:0.00612\teval-merror:0.07340\n",
            "[43]\ttrain-merror:0.00592\teval-merror:0.07340\n",
            "[44]\ttrain-merror:0.00570\teval-merror:0.07370\n",
            "[45]\ttrain-merror:0.00558\teval-merror:0.07370\n",
            "[46]\ttrain-merror:0.00542\teval-merror:0.07360\n",
            "[47]\ttrain-merror:0.00526\teval-merror:0.07340\n",
            "[48]\ttrain-merror:0.00512\teval-merror:0.07350\n",
            "[49]\ttrain-merror:0.00502\teval-merror:0.07330\n",
            "[50]\ttrain-merror:0.00496\teval-merror:0.07320\n",
            "[51]\ttrain-merror:0.00482\teval-merror:0.07310\n",
            "[52]\ttrain-merror:0.00458\teval-merror:0.07300\n",
            "[53]\ttrain-merror:0.00440\teval-merror:0.07280\n",
            "[54]\ttrain-merror:0.00430\teval-merror:0.07310\n",
            "[55]\ttrain-merror:0.00410\teval-merror:0.07290\n",
            "[56]\ttrain-merror:0.00394\teval-merror:0.07260\n",
            "[57]\ttrain-merror:0.00390\teval-merror:0.07240\n",
            "[58]\ttrain-merror:0.00374\teval-merror:0.07230\n",
            "[59]\ttrain-merror:0.00358\teval-merror:0.07240\n",
            "[60]\ttrain-merror:0.00350\teval-merror:0.07250\n",
            "[61]\ttrain-merror:0.00338\teval-merror:0.07250\n",
            "[62]\ttrain-merror:0.00324\teval-merror:0.07240\n",
            "[63]\ttrain-merror:0.00312\teval-merror:0.07230\n",
            "[64]\ttrain-merror:0.00306\teval-merror:0.07230\n",
            "[65]\ttrain-merror:0.00300\teval-merror:0.07230\n",
            "[66]\ttrain-merror:0.00294\teval-merror:0.07260\n",
            "[67]\ttrain-merror:0.00282\teval-merror:0.07230\n",
            "[68]\ttrain-merror:0.00274\teval-merror:0.07220\n",
            "[69]\ttrain-merror:0.00270\teval-merror:0.07210\n",
            "[70]\ttrain-merror:0.00266\teval-merror:0.07200\n",
            "[71]\ttrain-merror:0.00254\teval-merror:0.07210\n",
            "[72]\ttrain-merror:0.00254\teval-merror:0.07210\n",
            "[73]\ttrain-merror:0.00248\teval-merror:0.07190\n",
            "[74]\ttrain-merror:0.00240\teval-merror:0.07170\n",
            "[75]\ttrain-merror:0.00232\teval-merror:0.07170\n",
            "[76]\ttrain-merror:0.00224\teval-merror:0.07180\n",
            "[77]\ttrain-merror:0.00212\teval-merror:0.07170\n",
            "[78]\ttrain-merror:0.00208\teval-merror:0.07170\n",
            "[79]\ttrain-merror:0.00200\teval-merror:0.07190\n",
            "[80]\ttrain-merror:0.00192\teval-merror:0.07180\n",
            "[81]\ttrain-merror:0.00184\teval-merror:0.07180\n",
            "[82]\ttrain-merror:0.00174\teval-merror:0.07170\n",
            "[83]\ttrain-merror:0.00166\teval-merror:0.07180\n",
            "[84]\ttrain-merror:0.00162\teval-merror:0.07200\n",
            "[85]\ttrain-merror:0.00152\teval-merror:0.07220\n",
            "[86]\ttrain-merror:0.00148\teval-merror:0.07200\n",
            "[87]\ttrain-merror:0.00142\teval-merror:0.07210\n",
            "[88]\ttrain-merror:0.00132\teval-merror:0.07210\n",
            "[89]\ttrain-merror:0.00128\teval-merror:0.07220\n",
            "[90]\ttrain-merror:0.00118\teval-merror:0.07220\n",
            "[91]\ttrain-merror:0.00118\teval-merror:0.07220\n",
            "[92]\ttrain-merror:0.00112\teval-merror:0.07230\n",
            "[93]\ttrain-merror:0.00110\teval-merror:0.07230\n",
            "[94]\ttrain-merror:0.00106\teval-merror:0.07210\n",
            "[95]\ttrain-merror:0.00102\teval-merror:0.07210\n",
            "[96]\ttrain-merror:0.00100\teval-merror:0.07200\n",
            "[97]\ttrain-merror:0.00100\teval-merror:0.07200\n",
            "[98]\ttrain-merror:0.00098\teval-merror:0.07190\n",
            "[99]\ttrain-merror:0.00096\teval-merror:0.07180\n",
            "[100]\ttrain-merror:0.00094\teval-merror:0.07190\n",
            "[101]\ttrain-merror:0.00090\teval-merror:0.07200\n",
            "[102]\ttrain-merror:0.00086\teval-merror:0.07210\n",
            "[103]\ttrain-merror:0.00082\teval-merror:0.07210\n",
            "[104]\ttrain-merror:0.00076\teval-merror:0.07210\n",
            "[105]\ttrain-merror:0.00072\teval-merror:0.07220\n",
            "[106]\ttrain-merror:0.00064\teval-merror:0.07220\n",
            "[107]\ttrain-merror:0.00064\teval-merror:0.07220\n",
            "[108]\ttrain-merror:0.00052\teval-merror:0.07210\n",
            "[109]\ttrain-merror:0.00052\teval-merror:0.07190\n",
            "[110]\ttrain-merror:0.00048\teval-merror:0.07210\n",
            "[111]\ttrain-merror:0.00046\teval-merror:0.07210\n",
            "[112]\ttrain-merror:0.00046\teval-merror:0.07200\n",
            "[113]\ttrain-merror:0.00044\teval-merror:0.07220\n",
            "[114]\ttrain-merror:0.00044\teval-merror:0.07230\n",
            "[115]\ttrain-merror:0.00042\teval-merror:0.07210\n",
            "[116]\ttrain-merror:0.00042\teval-merror:0.07220\n",
            "[117]\ttrain-merror:0.00036\teval-merror:0.07230\n",
            "[118]\ttrain-merror:0.00032\teval-merror:0.07240\n",
            "[119]\ttrain-merror:0.00032\teval-merror:0.07240\n",
            "[120]\ttrain-merror:0.00032\teval-merror:0.07240\n",
            "[121]\ttrain-merror:0.00032\teval-merror:0.07240\n",
            "[122]\ttrain-merror:0.00028\teval-merror:0.07240\n",
            "[123]\ttrain-merror:0.00028\teval-merror:0.07220\n",
            "[124]\ttrain-merror:0.00028\teval-merror:0.07220\n",
            "[125]\ttrain-merror:0.00028\teval-merror:0.07230\n",
            "[126]\ttrain-merror:0.00028\teval-merror:0.07230\n",
            "[127]\ttrain-merror:0.00026\teval-merror:0.07250\n",
            "[128]\ttrain-merror:0.00026\teval-merror:0.07260\n",
            "[129]\ttrain-merror:0.00026\teval-merror:0.07250\n",
            "[130]\ttrain-merror:0.00024\teval-merror:0.07240\n",
            "[131]\ttrain-merror:0.00024\teval-merror:0.07240\n",
            "[132]\ttrain-merror:0.00024\teval-merror:0.07230\n",
            "[133]\ttrain-merror:0.00024\teval-merror:0.07230\n",
            "[134]\ttrain-merror:0.00022\teval-merror:0.07240\n",
            "[135]\ttrain-merror:0.00020\teval-merror:0.07240\n",
            "[136]\ttrain-merror:0.00020\teval-merror:0.07240\n",
            "[137]\ttrain-merror:0.00020\teval-merror:0.07240\n",
            "[138]\ttrain-merror:0.00020\teval-merror:0.07210\n",
            "[139]\ttrain-merror:0.00020\teval-merror:0.07230\n",
            "[140]\ttrain-merror:0.00020\teval-merror:0.07220\n",
            "[141]\ttrain-merror:0.00020\teval-merror:0.07210\n",
            "[142]\ttrain-merror:0.00020\teval-merror:0.07200\n",
            "[143]\ttrain-merror:0.00016\teval-merror:0.07200\n",
            "[144]\ttrain-merror:0.00016\teval-merror:0.07220\n",
            "[145]\ttrain-merror:0.00016\teval-merror:0.07210\n",
            "[146]\ttrain-merror:0.00016\teval-merror:0.07200\n",
            "[147]\ttrain-merror:0.00016\teval-merror:0.07180\n",
            "[148]\ttrain-merror:0.00016\teval-merror:0.07180\n",
            "[149]\ttrain-merror:0.00016\teval-merror:0.07180\n",
            "Build and save of XGBoost Model\n",
            "Accuracy: 0.9282\n",
            "Precision: 0.9284045923554484\n",
            "Recall: 0.9282\n",
            "F1 Score: 0.9282579490378573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzw3_oSBshjK"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import xgboost as xgb\n",
        "# from keras.models import model_from_json\n",
        "# from keras import optimizers\n",
        "# from keras.datasets import cifar10\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# import pickle\n",
        "\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# def load_cnn_model():\n",
        "#     json_file = open('model_resnet.json', 'r')\n",
        "#     loaded_model_json = json_file.read()\n",
        "#     json_file.close()\n",
        "#     model = model_from_json(loaded_model_json)\n",
        "#     # model = model_from_json(loaded_model_json)\n",
        "#     model.load_weights('model_resnet.weights.h5')\n",
        "#     model.compile(\n",
        "#         # optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "#         loss='categorical_crossentropy',\n",
        "#         metrics=['accuracy']\n",
        "#     )\n",
        "\n",
        "#     return model\n",
        "\n",
        "# def get_feature_layer(model, data):\n",
        "#     # Access the feature layer by name instead of index\n",
        "#     feature_layer_model = keras.Model(\n",
        "#         inputs=model.input,\n",
        "#         outputs=model.get_layer(name='flatten').output # Change here\n",
        "#     )\n",
        "#     feature_layer_output = feature_layer_model.predict(data)\n",
        "\n",
        "#     return feature_layer_output\n",
        "\n",
        "# def xgb_model(X_train, y_train, X_test, y_test):\n",
        "\n",
        "#     y_train = y_train.flatten()\n",
        "#     y_test = y_test.flatten()\n",
        "\n",
        "#     dtrain = xgb.DMatrix(\n",
        "#         X_train,\n",
        "#         label=y_train\n",
        "#     )\n",
        "\n",
        "#     dtest = xgb.DMatrix(\n",
        "#         X_test,\n",
        "#         label=y_test\n",
        "#     )\n",
        "\n",
        "#     params = {\n",
        "#         # 'max_depth': 12,\n",
        "#         # 'eta': 0.05,\n",
        "#         'objective': 'multi:softprob',\n",
        "#         'num_class': 10,\n",
        "#         'early_stopping_rounds': 5,\n",
        "#         'eval_metric': 'merror'\n",
        "#     }\n",
        "\n",
        "#     watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
        "#     # n_rounds = 100\n",
        "\n",
        "#     model = xgb.train(\n",
        "#         params,\n",
        "#         dtrain,\n",
        "#         # n_rounds,\n",
        "#         evals=watchlist\n",
        "#     )\n",
        "\n",
        "#     return model\n",
        "\n",
        "# def evaluate_model(model, X_test, y_test):\n",
        "#     dtest = xgb.DMatrix(X_test)\n",
        "#     y_pred_prob = model.predict(dtest)\n",
        "#     y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "#     precision = precision_score(y_test, y_pred, average='macro')\n",
        "#     recall = recall_score(y_test, y_pred, average='macro')\n",
        "#     f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "#     print(f'Accuracy: {accuracy}')\n",
        "#     print(f'Precision: {precision}')\n",
        "#     print(f'Recall: {recall}')\n",
        "#     print(f'F1 Score: {f1}')\n",
        "\n",
        "#     return accuracy, precision, recall, f1\n",
        "\n",
        "# def main():\n",
        "#     # Load CIFAR-10 dataset\n",
        "#     (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#     indices = np.random.permutation(X_train.shape[0])\n",
        "#     train_size = 40000  # Ubah sesuai kebutuhan\n",
        "#     X_train = X_train[indices][:train_size]\n",
        "#     y_train = y_train[indices][:train_size]\n",
        "\n",
        "#     # Normalize the data\n",
        "#     X_train = X_train.astype('float32') / 255.0\n",
        "#     X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "#     y_train = y_train.flatten()\n",
        "#     y_test = y_test.flatten()\n",
        "\n",
        "#     # Load the CNN model and extract features\n",
        "#     cnn_model = load_cnn_model()\n",
        "\n",
        "#     X_train_cnn = get_feature_layer(cnn_model, X_train)\n",
        "#     X_test_cnn = get_feature_layer(cnn_model, X_test)\n",
        "\n",
        "#     # Train XGBoost model\n",
        "#     model = xgb_model(X_train_cnn, y_train, X_test_cnn, y_test)\n",
        "\n",
        "#     print(\"Build and save of XGBoost Model\")\n",
        "#     pickle.dump(model, open(\"cnn_xgboost_resnet_final.pickle.dat\", \"wb\"))\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     evaluate_model(model, X_test_cnn, y_test)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9j5z0rNUkPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84e6c14-98d9-4350-8022-e67011e87956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250/1250 [==============================] - 96s 76ms/step\n",
            "313/313 [==============================] - 25s 76ms/step\n",
            "313/313 [==============================] - 25s 76ms/step\n",
            "100%|██████████| 100/100 [40:07<00:00, 24.07s/trial, best loss: -0.9713]\n",
            "Best hyperparameters found were: {'base_score': 0.20140759947016906, 'colsample_bylevel': 0.5699552760093669, 'colsample_bynode': 0.9603035736706265, 'colsample_bytree': 0.24849516570410665, 'gamma': 0.06787305164081797, 'learning_rate': 0.07566773957128736, 'max_delta_step': 0.9108614334646671, 'max_depth': 9.0, 'min_child_weight': 1.1573277177023775, 'num_parallel_tree': 8.0, 'subsample': 0.9449987747289236}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:28:58] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9285\n",
            "Precision: 0.9286319761570389\n",
            "Recall: 0.9285\n",
            "F1 Score: 0.9285282972731904\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from xgboost.callback import TrainingCallback\n",
        "import keras\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "def load_cnn_model():\n",
        "    json_file = open('model_resnet.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    model = model_from_json(loaded_model_json)\n",
        "    model.load_weights('model_resnet.h5')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_feature_layer(model, data):\n",
        "    total_layers = len(model.layers)\n",
        "    fl_index = total_layers - 7  # Get the correct index for the feature layer\n",
        "\n",
        "    feature_layer_model = keras.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=model.get_layer(index=fl_index).output\n",
        "    )\n",
        "    feature_layer_output = feature_layer_model.predict(data)\n",
        "\n",
        "    return feature_layer_output\n",
        "\n",
        "def objective(space, X_train_cnn, y_train, X_valid_cnn, y_valid):\n",
        "    params = {\n",
        "        'learning_rate': space['learning_rate'],\n",
        "        'max_depth': int(space['max_depth']),\n",
        "        'min_child_weight': space['min_child_weight'],\n",
        "        'subsample': space['subsample'],\n",
        "        'colsample_bytree': space['colsample_bytree'],\n",
        "        'base_score': space['base_score'],\n",
        "        'colsample_bylevel': space['colsample_bylevel'],\n",
        "        'colsample_bynode': space['colsample_bynode'],\n",
        "        'num_parallel_tree': int(space['num_parallel_tree']),\n",
        "        'max_delta_step': space['max_delta_step'],\n",
        "        'gamma': space['gamma'],\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': 10,\n",
        "        'eval_metric': 'merror'\n",
        "    }\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train_cnn, label=y_train)\n",
        "    dvalid = xgb.DMatrix(X_valid_cnn, label=y_valid)\n",
        "\n",
        "    evals_result = {}\n",
        "    model = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dvalid, 'eval')],\n",
        "                      early_stopping_rounds=10, evals_result=evals_result, verbose_eval=False)\n",
        "\n",
        "    pred = model.predict(dvalid)\n",
        "    pred_labels = np.argmax(pred, axis=1)\n",
        "    accuracy = accuracy_score(y_valid, pred_labels)\n",
        "\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
        "\n",
        "def evaluate_model(model, X_test_cnn, y_test):\n",
        "    dtest = xgb.DMatrix(X_test_cnn)\n",
        "    pred = model.predict(dtest)\n",
        "    pred_labels = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, pred_labels)\n",
        "    precision = precision_score(y_test, pred_labels, average='weighted')\n",
        "    recall = recall_score(y_test, pred_labels, average='weighted')\n",
        "    f1 = f1_score(y_test, pred_labels, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "\n",
        "class EarlyStopCallback(TrainingCallback):\n",
        "    def __init__(self, threshold):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def after_iteration(self, model, epoch, evals_log):\n",
        "        for eval_name, eval_metrics in evals_log.items():\n",
        "            for metric_name, metric_values in eval_metrics.items():\n",
        "                if eval_name == 'eval' and metric_name == 'merror' and 1 - metric_values[-1] > self.threshold:\n",
        "                    print(f\"Stopping early. Accuracy reached {1 - metric_values[-1]}\")\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    # Load CIFAR-10 dataset\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    # Preprocess data: normalize images\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "    # Data Augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.2,\n",
        "        shear_range=0.2\n",
        "    )\n",
        "\n",
        "    datagen.fit(X_train)\n",
        "\n",
        "    # Convert labels to single-dimensional integers\n",
        "    y_train = y_train.flatten()\n",
        "    y_test = y_test.flatten()\n",
        "\n",
        "    # Split training data into training and validation sets\n",
        "    train_size = int(0.8 * len(X_train))\n",
        "    X_train, X_valid = X_train[:train_size], X_train[train_size:]\n",
        "    y_train, y_valid = y_train[:train_size], y_train[train_size:]\n",
        "\n",
        "    # Load the CNN model and extract features\n",
        "    cnn_model = load_cnn_model()\n",
        "\n",
        "    X_train_cnn = get_feature_layer(cnn_model, X_train)\n",
        "    X_valid_cnn = get_feature_layer(cnn_model, X_valid)\n",
        "    X_test_cnn = get_feature_layer(cnn_model, X_test)\n",
        "\n",
        "    # Define the search space\n",
        "    space = {\n",
        "        'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
        "        'max_depth': hp.quniform('max_depth', 1, 15, 1),\n",
        "        'min_child_weight': hp.uniform('min_child_weight', 0, 5),\n",
        "        'subsample': hp.uniform('subsample', 0.1, 1),\n",
        "        'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1),\n",
        "        'base_score': hp.uniform('base_score', 0.1, 0.9),\n",
        "        'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1),\n",
        "        'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1),\n",
        "        'max_delta_step': hp.uniform('max_delta_step', 0, 1),\n",
        "        'num_parallel_tree': hp.quniform('num_parallel_tree', 1, 10, 1),\n",
        "        'gamma': hp.uniform('gamma', 0, 0.1)\n",
        "    }\n",
        "\n",
        "    # Run the optimization\n",
        "    trials = Trials()\n",
        "    best = fmin(\n",
        "        fn=lambda space: objective(space, X_train_cnn, y_train, X_valid_cnn, y_valid),\n",
        "        space=space,\n",
        "        algo=tpe.suggest,\n",
        "        max_evals=100,\n",
        "        trials=trials\n",
        "    )\n",
        "\n",
        "    print(\"Best hyperparameters found were:\", best)\n",
        "\n",
        "    # Train the final model with the best hyperparameters\n",
        "    best_params = {\n",
        "        'learning_rate': best['learning_rate'],\n",
        "        'max_depth': int(best['max_depth']),\n",
        "        'min_child_weight': best['min_child_weight'],\n",
        "        'subsample': best['subsample'],\n",
        "        'colsample_bytree': best['colsample_bytree'],\n",
        "        'base_score': best['base_score'],\n",
        "        'colsample_bylevel': best['colsample_bylevel'],\n",
        "        'colsample_bynode': best['colsample_bynode'],\n",
        "        'num_parallel_tree': int(best['num_parallel_tree']),\n",
        "        'max_delta_step': best['max_delta_step'],\n",
        "        'gamma': best['gamma'],\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': 10,\n",
        "        'eval_metric': 'merror',\n",
        "        'booster': 'gbtree'\n",
        "    }\n",
        "\n",
        "    # Combine training and validation data\n",
        "    X_combined_cnn = np.vstack((X_train_cnn, X_valid_cnn))\n",
        "    y_combined = np.hstack((y_train, y_valid))\n",
        "\n",
        "    dtrain_combined = xgb.DMatrix(X_combined_cnn, label=y_combined)\n",
        "\n",
        "    # Define custom callback\n",
        "    early_stop_callback = EarlyStopCallback(threshold=0.948)\n",
        "\n",
        "    # Train the final model\n",
        "    final_model = xgb.train(best_params, dtrain_combined, num_boost_round=1000, callbacks=[early_stop_callback])\n",
        "\n",
        "    # Save the final model\n",
        "    final_model.save_model('optimized_xgboost_model.dat')\n",
        "\n",
        "    # Evaluate the final model\n",
        "    evaluate_model(final_model, X_test_cnn, y_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG4or2E5GtfK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2LExrlTNe-gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-v-E3xSb2X1"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import pickle\n",
        "# import xgboost as xgb\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from tensorflow.keras.datasets import cifar10\n",
        "# from tensorflow.keras.models import model_from_json\n",
        "# from tensorflow.keras import optimizers\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# import keras\n",
        "\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# def load_cnn_model():\n",
        "#     json_file = open('model_resnet.json', 'r')\n",
        "#     loaded_model_json = json_file.read()\n",
        "#     json_file.close()\n",
        "#     model = model_from_json(loaded_model_json)\n",
        "#     model.load_weights('model_resnet.h5')  # load weights into new model\n",
        "\n",
        "#     model.compile(\n",
        "#         optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "#         loss='categorical_crossentropy',\n",
        "#         metrics=['accuracy']\n",
        "#     )\n",
        "\n",
        "#     return model\n",
        "\n",
        "# def get_feature_layer(model, data):\n",
        "#     total_layers = len(model.layers)\n",
        "#     fl_index = total_layers - 7  # Get the correct index for the feature layer\n",
        "\n",
        "#     feature_layer_model = keras.Model(\n",
        "#         inputs=model.input,\n",
        "#         outputs=model.get_layer(index=fl_index).output\n",
        "#     )\n",
        "#     feature_layer_output = feature_layer_model.predict(data)\n",
        "\n",
        "#     return feature_layer_output\n",
        "\n",
        "# def evaluate_model(model, X_test_cnn, y_test):\n",
        "#     dtest = xgb.DMatrix(X_test_cnn)\n",
        "#     pred = model.predict(dtest)\n",
        "#     pred_labels = np.argmax(pred, axis=1)\n",
        "\n",
        "#     accuracy = accuracy_score(y_test, pred_labels)\n",
        "#     precision = precision_score(y_test, pred_labels, average='weighted')\n",
        "#     recall = recall_score(y_test, pred_labels, average='weighted')\n",
        "#     f1 = f1_score(y_test, pred_labels, average='weighted')\n",
        "\n",
        "#     print(f\"Accuracy: {accuracy}\")\n",
        "#     print(f\"Precision: {precision}\")\n",
        "#     print(f\"Recall: {recall}\")\n",
        "#     print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# def main():\n",
        "#     # Load CIFAR-10 dataset\n",
        "#     (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#     # Preprocess data: normalize images\n",
        "#     X_train = X_train.astype('float32') / 255.0\n",
        "#     X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "#     # Data Augmentation\n",
        "#     datagen = ImageDataGenerator(\n",
        "#         rotation_range=15,\n",
        "#         width_shift_range=0.1,\n",
        "#         height_shift_range=0.1,\n",
        "#         horizontal_flip=True\n",
        "#     )\n",
        "\n",
        "#     datagen.fit(X_train)\n",
        "\n",
        "#     # Convert labels to single-dimensional integers\n",
        "#     y_train = y_train.flatten()\n",
        "#     y_test = y_test.flatten()\n",
        "\n",
        "#     # Split training data into training and validation sets\n",
        "#     train_size = int(0.8 * len(X_train))\n",
        "#     X_train, X_valid = X_train[:train_size], X_train[train_size:]\n",
        "#     y_train, y_valid = y_train[:train_size], y_train[train_size:]\n",
        "\n",
        "#     # Load the CNN model and extract features\n",
        "#     cnn_model = load_cnn_model()\n",
        "\n",
        "#     X_train_cnn = get_feature_layer(cnn_model, X_train)\n",
        "#     X_valid_cnn = get_feature_layer(cnn_model, X_valid)\n",
        "#     X_test_cnn = get_feature_layer(cnn_model, X_test)\n",
        "\n",
        "#     # Combine training and validation data\n",
        "#     X_combined_cnn = np.vstack((X_train_cnn, X_valid_cnn))\n",
        "#     y_combined = np.hstack((y_train, y_valid))\n",
        "\n",
        "#     # Define the parameter grid\n",
        "#     param_grid = {\n",
        "#         'learning_rate': [0.01, 0.1, 0.2],\n",
        "#         'max_depth': [6, 10, 15],\n",
        "#         'min_child_weight': [1, 5, 10],\n",
        "#         'subsample': [0.8, 0.9, 1.0],\n",
        "#         'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "#         'n_estimators': [100, 300, 500]\n",
        "#     }\n",
        "\n",
        "#     # Create DMatrix for combined training data\n",
        "#     dtrain_combined = xgb.DMatrix(X_combined_cnn, label=y_combined)\n",
        "\n",
        "#     # Initialize XGBoost model\n",
        "#     xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=10)\n",
        "\n",
        "#     # Perform grid search\n",
        "#     grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
        "#     grid_search.fit(X_combined_cnn, y_combined)\n",
        "\n",
        "#     # Get the best parameters and best model\n",
        "#     best_params = grid_search.best_params_\n",
        "#     print(\"Best hyperparameters found were:\", best_params)\n",
        "\n",
        "#     # Train the final model with the best hyperparameters\n",
        "#     final_model = xgb.XGBClassifier(**best_params, objective='multi:softprob', num_class=10)\n",
        "#     final_model.fit(X_combined_cnn, y_combined)\n",
        "\n",
        "#     # Save the final model\n",
        "#     pickle.dump(final_model, open('optimized_xgboost_model_gridsearch.pkl', 'wb'))\n",
        "\n",
        "#     # Evaluate the final model\n",
        "#     evaluate_model(final_model.get_booster(), X_test_cnn, y_test)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChalvlS51Ylf",
        "outputId": "a9c1d471-de55-4410-c4bd-a26fac3918a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 - 5s - loss: 1.5076 - accuracy: 0.4583 - val_loss: 1.2711 - val_accuracy: 0.5476 - 5s/epoch - 8ms/step\n",
            "Epoch 2/10\n",
            "625/625 - 2s - loss: 1.1701 - accuracy: 0.5875 - val_loss: 1.1077 - val_accuracy: 0.6141 - 2s/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "625/625 - 2s - loss: 1.0403 - accuracy: 0.6382 - val_loss: 1.0798 - val_accuracy: 0.6217 - 2s/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "625/625 - 2s - loss: 0.9492 - accuracy: 0.6699 - val_loss: 1.0164 - val_accuracy: 0.6503 - 2s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "625/625 - 2s - loss: 0.8681 - accuracy: 0.6965 - val_loss: 0.9997 - val_accuracy: 0.6595 - 2s/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "625/625 - 2s - loss: 0.8105 - accuracy: 0.7170 - val_loss: 0.9350 - val_accuracy: 0.6772 - 2s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "625/625 - 2s - loss: 0.7457 - accuracy: 0.7401 - val_loss: 0.9222 - val_accuracy: 0.6850 - 2s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "625/625 - 2s - loss: 0.6897 - accuracy: 0.7576 - val_loss: 0.9130 - val_accuracy: 0.6911 - 2s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "625/625 - 2s - loss: 0.6369 - accuracy: 0.7771 - val_loss: 0.9286 - val_accuracy: 0.6858 - 2s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "625/625 - 2s - loss: 0.5832 - accuracy: 0.7962 - val_loss: 0.9634 - val_accuracy: 0.6870 - 2s/epoch - 4ms/step\n",
            "1563/1563 [==============================] - 3s 2ms/step\n",
            "313/313 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:29:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6939\n",
            "Precision: 0.6932599306828658\n",
            "Recall: 0.6939\n",
            "F1 Score: 0.6934003579702347\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# 2. Preprocess data: normalize images and flatten labels\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Flatten labels\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "\n",
        "# 3. Define a function to extract features using a simple CNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "def create_feature_extractor():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train a simple CNN to extract features\n",
        "feature_extractor = create_feature_extractor()\n",
        "feature_extractor.fit(X_train, to_categorical(y_train), epochs=10, batch_size=64, validation_split=0.2, verbose=2)\n",
        "\n",
        "# Remove the last layer to get the feature extractor\n",
        "feature_extractor = Sequential(feature_extractor.layers[:-1])\n",
        "\n",
        "# 4. Extract features from the training and test data\n",
        "X_train_features = feature_extractor.predict(X_train)\n",
        "X_test_features = feature_extractor.predict(X_test)\n",
        "\n",
        "# 5. Train XGBoost model on extracted features\n",
        "dtrain = xgb.DMatrix(X_train_features, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test_features, label=y_test)\n",
        "\n",
        "params = {\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': 10,\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100\n",
        "}\n",
        "\n",
        "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# 6. Predict and evaluate the model\n",
        "y_pred = bst.predict(dtest)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_labels)\n",
        "precision = precision_score(y_test, y_pred_labels, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_labels, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_labels, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}