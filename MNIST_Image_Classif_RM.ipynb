{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Check the current version of Keras\n",
        "!pip show tensorflow\n",
        "!pip show keras\n",
        "\n",
        "# Uninstall the current version of Keras\n",
        "!pip uninstall tensorflow -y\n",
        "!pip uninstall keras -y\n",
        "\n",
        "# Install the specific version of Keras\n",
        "!pip install tensorflow==2.15.1\n",
        "# !pip install keras==2.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y3ad6G3Tb6ha",
        "outputId": "2987c0ed-5c07-4b65-d0dd-2f7f2143f42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.17.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine_rl, tf_keras\n",
            "Name: keras\n",
            "Version: 3.4.1\n",
            "Summary: Multi-backend Keras.\n",
            "Home-page: https://github.com/keras-team/keras\n",
            "Author: Keras team\n",
            "Author-email: keras-users@googlegroups.com\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, h5py, ml-dtypes, namex, numpy, optree, packaging, rich\n",
            "Required-by: tensorflow\n",
            "Found existing installation: tensorflow 2.17.0\n",
            "Uninstalling tensorflow-2.17.0:\n",
            "  Successfully uninstalled tensorflow-2.17.0\n",
            "Found existing installation: keras 3.4.1\n",
            "Uninstalling keras-3.4.1:\n",
            "  Successfully uninstalled keras-3.4.1\n",
            "Collecting tensorflow==2.15.1\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.15.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.1)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.1)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n",
            "Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.3.2 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "ml_dtypes",
                  "tensorflow",
                  "wrapt"
                ]
              },
              "id": "1e2fef50b1104f49ae381d4b0f8e3688"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "\n",
        "import keras\n",
        "print(keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Pn8KYJ9cTQE",
        "outputId": "21793074-676a-4c0c-ec5a-ad52fcaf59ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.1\n",
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kW-sHh-caQW",
        "outputId": "3c086599-082d-440b-83b0-ddd119107529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcBf7lRebaEZ",
        "outputId": "8699f892-150c-4981-fd66-215ea587fa66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 103s 54ms/step - loss: 0.4517 - accuracy: 0.8335 - val_loss: 0.3237 - val_accuracy: 0.8854\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 102s 54ms/step - loss: 0.2879 - accuracy: 0.8948 - val_loss: 0.2763 - val_accuracy: 0.8993\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 110s 59ms/step - loss: 0.2475 - accuracy: 0.9100 - val_loss: 0.2661 - val_accuracy: 0.8989\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 102s 54ms/step - loss: 0.2176 - accuracy: 0.9196 - val_loss: 0.2324 - val_accuracy: 0.9150\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 102s 55ms/step - loss: 0.1946 - accuracy: 0.9279 - val_loss: 0.2403 - val_accuracy: 0.9114\n",
            "1875/1875 [==============================] - 29s 16ms/step\n",
            "313/313 [==============================] - 5s 16ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-07 08:56:04,059] A new study created in memory with name: no-name-6f3fefdd-2be7-4b10-aad9-22db6dd16c6a\n",
            "<ipython-input-7-cf1afe269f54>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
            "<ipython-input-7-cf1afe269f54>:62: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
            "<ipython-input-7-cf1afe269f54>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
            "[I 2024-08-07 09:36:31,904] Trial 0 finished with value: 0.9224 and parameters: {'max_depth': 16, 'learning_rate': 0.018468149980538435, 'n_estimators': 782, 'subsample': 0.598618072442087, 'colsample_bytree': 0.9035957983867994}. Best is trial 0 with value: 0.9224.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Accuracy: 0.9224\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top       0.87      0.88      0.88      1000\n",
            "     Trouser       0.99      0.98      0.99      1000\n",
            "    Pullover       0.87      0.88      0.88      1000\n",
            "       Dress       0.92      0.93      0.93      1000\n",
            "        Coat       0.88      0.87      0.88      1000\n",
            "      Sandal       0.98      0.99      0.99      1000\n",
            "       Shirt       0.77      0.76      0.77      1000\n",
            "     Sneaker       0.96      0.98      0.97      1000\n",
            "         Bag       0.98      0.97      0.98      1000\n",
            "  Ankle boot       0.98      0.97      0.98      1000\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.92      0.92      0.92     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.metrics import classification_report, accuracy_score\n",
        "# import numpy as np\n",
        "# from keras.datasets import fashion_mnist\n",
        "# from keras.utils import to_categorical\n",
        "# from keras.models import Sequential, Model\n",
        "# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# import optuna\n",
        "# import xgboost as xgb\n",
        "\n",
        "# # Load Fashion MNIST data\n",
        "# (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# # Preprocess the data\n",
        "# train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "# test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# # Convert labels to categorical\n",
        "# train_labels = to_categorical(train_labels)\n",
        "# test_labels = to_categorical(test_labels)\n",
        "\n",
        "# # Build the model\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(512, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Fit the model\n",
        "# model.fit(train_images, train_labels, epochs=5, verbose=1, validation_data=(test_images, test_labels))\n",
        "\n",
        "# # Create a feature extractor model\n",
        "# feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "# # Function to extract features\n",
        "# def extract_features(images, model):\n",
        "#     return model.predict(images)\n",
        "\n",
        "# # Extract features for training and test sets\n",
        "# train_features = extract_features(train_images, feature_extractor)\n",
        "# test_features = extract_features(test_images, feature_extractor)\n",
        "\n",
        "# # Convert labels to one-dimensional array\n",
        "# train_labels = np.argmax(train_labels, axis=1)\n",
        "# test_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# # Define objective function for Optuna\n",
        "# def objective(trial):\n",
        "#     param = {\n",
        "#         'objective': 'multi:softmax',\n",
        "#         'num_class': 10,\n",
        "#         'max_depth': trial.suggest_int('max_depth', 4, 16),\n",
        "#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
        "#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "#         'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
        "#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
        "#     }\n",
        "\n",
        "#     model = xgb.XGBClassifier(**param)\n",
        "#     model.fit(train_features, train_labels)\n",
        "\n",
        "#     preds = model.predict(test_features)\n",
        "#     accuracy = accuracy_score(test_labels, preds)\n",
        "#     return accuracy\n",
        "\n",
        "# # Create study and optimize\n",
        "# study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
        "# study.optimize(objective, n_trials=1)\n",
        "\n",
        "# # Train the final model with the best parameters\n",
        "# best_params = study.best_params\n",
        "# final_model = xgb.XGBClassifier(**best_params)\n",
        "# final_model.fit(train_features, train_labels)\n",
        "\n",
        "# # Evaluate the model\n",
        "# preds = final_model.predict(test_features)\n",
        "# accuracy = accuracy_score(test_labels, preds)\n",
        "# print(f'Final Model Accuracy: {accuracy}')\n",
        "\n",
        "# # Generate classification report\n",
        "# report = classification_report(test_labels, preds, target_names=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"])\n",
        "# print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Load Fashion MNIST data\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to categorical\n",
        "train_labels_cat = to_categorical(train_labels)\n",
        "test_labels_cat = to_categorical(test_labels)\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the CNN model\n",
        "model.fit(train_images, train_labels_cat, epochs=5, verbose=1, validation_data=(test_images, test_labels_cat))\n",
        "\n",
        "# Save the CNN model\n",
        "model.save('cnn_fashion_mnist.h5')\n",
        "\n",
        "# Evaluate CNN model\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FQiVhCCXvde",
        "outputId": "e62f5ceb-6df9-4bb0-c2eb-b840efd47393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 27s 9ms/step - loss: 0.4663 - accuracy: 0.8290 - val_loss: 0.3272 - val_accuracy: 0.8768\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2920 - accuracy: 0.8942 - val_loss: 0.2850 - val_accuracy: 0.8940\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2479 - accuracy: 0.9089 - val_loss: 0.2553 - val_accuracy: 0.9026\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2186 - accuracy: 0.9199 - val_loss: 0.2520 - val_accuracy: 0.9093\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1929 - accuracy: 0.9285 - val_loss: 0.2432 - val_accuracy: 0.9136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from keras.models import load_model, Model\n",
        "import xgboost as xgb\n",
        "\n",
        "cnn_preds = model.predict(test_images)\n",
        "cnn_preds_classes = np.argmax(cnn_preds, axis=1)\n",
        "cnn_accuracy = accuracy_score(test_labels, cnn_preds_classes)\n",
        "cnn_precision = precision_score(test_labels, cnn_preds_classes, average='weighted')\n",
        "cnn_recall = recall_score(test_labels, cnn_preds_classes, average='weighted')\n",
        "cnn_f1 = f1_score(test_labels, cnn_preds_classes, average='weighted')\n",
        "print(f'CNN Accuracy: {cnn_accuracy}')\n",
        "print(f'CNN Precision: {cnn_precision}')\n",
        "print(f'CNN Recall: {cnn_recall}')\n",
        "print(f'CNN F1-Score: {cnn_f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbXVFIITY4wx",
        "outputId": "34a317fd-12a2-4de5-fd10-457f7b9f1a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n",
            "CNN Accuracy: 0.9136\n",
            "CNN Precision: 0.9143068707280898\n",
            "CNN Recall: 0.9136\n",
            "CNN F1-Score: 0.9136706132638753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(images, model):\n",
        "    return model.predict(images)"
      ],
      "metadata": {
        "id": "FNJISk5WOndi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
      ],
      "metadata": {
        "id": "Xx6RvQEKOuEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = extract_features(train_images, feature_extractor)\n",
        "np.save('train_features.npy', train_features)\n",
        "np.save('train_labels.npy', train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO3xy-taZjLW",
        "outputId": "0498aa64-1274-4792-a1ec-83d67e15aaa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 3s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from keras.models import load_model, Model\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load Fashion MNIST data\n",
        "(_, _), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to categorical\n",
        "test_labels_cat = to_categorical(test_labels)\n",
        "\n",
        "# Load the CNN model\n",
        "model = load_model('cnn_fashion_mnist.h5')\n",
        "\n",
        "# Create a feature extractor model\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(images, model):\n",
        "    return model.predict(images)\n",
        "\n",
        "# Extract features for training and test sets\n",
        "train_features = np.load('train_features.npy')\n",
        "train_labels = np.load('train_labels.npy')\n",
        "test_features = extract_features(test_images, feature_extractor)\n",
        "\n",
        "# Convert labels to one-dimensional array\n",
        "test_labels_flat = np.argmax(test_labels_cat, axis=1)\n",
        "\n",
        "# Define XGBoost model parameters (without TPE)\n",
        "xgb_params = {\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': 10,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "}\n",
        "\n",
        "# Train the XGBoost model without TPE\n",
        "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "xgb_model.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate XGBoost model without TPE\n",
        "xgb_preds = xgb_model.predict(test_features)\n",
        "xgb_accuracy = accuracy_score(test_labels_flat, xgb_preds)\n",
        "xgb_precision = precision_score(test_labels_flat, xgb_preds, average='weighted')\n",
        "xgb_recall = recall_score(test_labels_flat, xgb_preds, average='weighted')\n",
        "xgb_f1 = f1_score(test_labels_flat, xgb_preds, average='weighted')\n",
        "print(f'XGBoost Accuracy: {xgb_accuracy}')\n",
        "print(f'XGBoost Precision: {xgb_precision}')\n",
        "print(f'XGBoost Recall: {xgb_recall}')\n",
        "print(f'XGBoost F1-Score: {xgb_f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg52moA5Ybmu",
        "outputId": "dc6e7785-3fdf-4c11-8715-b808bd4d370f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step\n",
            "XGBoost Accuracy: 0.9187\n",
            "XGBoost Precision: 0.9187516468805439\n",
            "XGBoost Recall: 0.9187\n",
            "XGBoost F1-Score: 0.9186968796379268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from keras.models import load_model, Model\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load Fashion MNIST data\n",
        "(_, _), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to categorical\n",
        "test_labels_cat = to_categorical(test_labels)\n",
        "\n",
        "# Load the CNN model\n",
        "model = load_model('cnn_fashion_mnist.h5')\n",
        "\n",
        "# Create a feature extractor model\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(images, model):\n",
        "    return model.predict(images)\n",
        "\n",
        "# Extract features for training and test sets\n",
        "train_features = np.load('train_features.npy')\n",
        "train_labels = np.load('train_labels.npy')\n",
        "test_features = extract_features(test_images, feature_extractor)\n",
        "\n",
        "# Convert labels to one-dimensional array\n",
        "test_labels_flat = np.argmax(test_labels_cat, axis=1)\n",
        "\n",
        "# Define objective function for Optuna (with TPE)\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'multi:softmax',\n",
        "        'num_class': 10,\n",
        "        'max_depth': trial.suggest_int('max_depth', 4, 16),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**param)\n",
        "    model.fit(train_features, train_labels)\n",
        "\n",
        "    preds = model.predict(test_features)\n",
        "    accuracy = accuracy_score(test_labels_flat, preds)\n",
        "    return accuracy\n",
        "\n",
        "# Create study and optimize\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective, n_trials=1)\n",
        "\n",
        "# Train the final model with the best parameters (with TPE)\n",
        "best_params = study.best_params\n",
        "final_model = xgb.XGBClassifier(**best_params)\n",
        "final_model.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate the model with TPE\n",
        "tpe_preds = final_model.predict(test_features)\n",
        "tpe_accuracy = accuracy_score(test_labels_flat, tpe_preds)\n",
        "tpe_precision = precision_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "tpe_recall = recall_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "tpe_f1 = f1_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "print(f'XGBoost with TPE Accuracy: {tpe_accuracy}')\n",
        "print(f'XGBoost with TPE Precision: {tpe_precision}')\n",
        "print(f'XGBoost with TPE Recall: {tpe_recall}')\n",
        "print(f'XGBoost with TPE F1-Score: {tpe_f1}')\n"
      ],
      "metadata": {
        "id": "NCe8mHPfYdhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load Fashion MNIST data\n",
        "(_, _), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to categorical\n",
        "test_labels_cat = to_categorical(test_labels)\n",
        "\n",
        "# Load the CNN model\n",
        "model = load_model('cnn_fashion_mnist.h5')\n",
        "\n",
        "# Create a feature extractor model\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(images, model):\n",
        "    return model.predict(images)\n",
        "\n",
        "# Extract features for training and test sets\n",
        "train_features = np.load('train_features.npy')\n",
        "train_labels = np.load('train_labels.npy')\n",
        "test_features = extract_features(test_images, feature_extractor)\n",
        "\n",
        "# Convert labels to one-dimensional array\n",
        "test_labels_flat = np.argmax(test_labels_cat, axis=1)\n",
        "\n",
        "# Define objective function for Optuna (with TPE)\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'multi:softmax',\n",
        "        'num_class': 10,\n",
        "        'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 15),\n",
        "        'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 5),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1),\n",
        "        'base_score': trial.suggest_uniform('base_score', 0.1, 0.9),\n",
        "        'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.1, 1),\n",
        "        'colsample_bynode': trial.suggest_uniform('colsample_bynode', 0.1, 1),\n",
        "        'max_delta_step': trial.suggest_uniform('max_delta_step', 0, 1),\n",
        "        'num_parallel_tree': trial.suggest_int('num_parallel_tree', 1, 10),\n",
        "        'gamma': trial.suggest_uniform('gamma', 0, 0.1),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**param)\n",
        "    model.fit(train_features, train_labels)\n",
        "\n",
        "    preds = model.predict(test_features)\n",
        "    accuracy = accuracy_score(test_labels_flat, preds)\n",
        "    return accuracy\n",
        "\n",
        "# Create study and optimize\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective, n_trials=1)\n",
        "\n",
        "# Train the final model with the best parameters (with TPE)\n",
        "best_params = study.best_params\n",
        "final_model = xgb.XGBClassifier(**best_params)\n",
        "final_model.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate the model with TPE\n",
        "tpe_preds = final_model.predict(test_features)\n",
        "tpe_accuracy = accuracy_score(test_labels_flat, tpe_preds)\n",
        "tpe_precision = precision_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "tpe_recall = recall_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "tpe_f1 = f1_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "print(f'XGBoost with TPE Accuracy: {tpe_accuracy}')\n",
        "print(f'XGBoost with TPE Precision: {tpe_precision}')\n",
        "print(f'XGBoost with TPE Recall: {tpe_recall}')\n",
        "print(f'XGBoost with TPE F1-Score: {tpe_f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4Kyl8mgYfv0",
        "outputId": "58e64377-f1a0-49ac-8e5f-5682a410e6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-07 23:51:14,580] A new study created in memory with name: no-name-0e65e8ea-0c02-4e77-96a3-ccd6ba99c413\n",
            "<ipython-input-11-74c8e0978498>:41: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-11-74c8e0978498>:43: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 5),\n",
            "<ipython-input-11-74c8e0978498>:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
            "<ipython-input-11-74c8e0978498>:45: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1),\n",
            "<ipython-input-11-74c8e0978498>:46: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'base_score': trial.suggest_uniform('base_score', 0.1, 0.9),\n",
            "<ipython-input-11-74c8e0978498>:47: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.1, 1),\n",
            "<ipython-input-11-74c8e0978498>:48: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bynode': trial.suggest_uniform('colsample_bynode', 0.1, 1),\n",
            "<ipython-input-11-74c8e0978498>:49: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'max_delta_step': trial.suggest_uniform('max_delta_step', 0, 1),\n",
            "<ipython-input-11-74c8e0978498>:51: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.1),\n",
            "[I 2024-08-08 01:14:18,552] Trial 0 finished with value: 0.9198 and parameters: {'learning_rate': 0.034738095185573345, 'max_depth': 12, 'min_child_weight': 2.7251833262535663, 'subsample': 0.3268596658325792, 'colsample_bytree': 0.7095410758842947, 'base_score': 0.24008575301562907, 'colsample_bylevel': 0.7719359386069632, 'colsample_bynode': 0.1102878470518675, 'max_delta_step': 0.4883774052410108, 'num_parallel_tree': 7, 'gamma': 0.005931762197591839, 'n_estimators': 402}. Best is trial 0 with value: 0.9198.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load Fashion MNIST data\n",
        "(_, _), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# Convert labels to categorical\n",
        "test_labels_cat = to_categorical(test_labels)\n",
        "\n",
        "# Load the CNN model\n",
        "model = load_model('cnn_fashion_mnist.h5')\n",
        "\n",
        "# Create a feature extractor model\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(images, model):\n",
        "    return model.predict(images)\n",
        "\n",
        "# Extract features for training and test sets\n",
        "train_features = np.load('train_features.npy')\n",
        "train_labels = np.load('train_labels.npy')\n",
        "test_features = extract_features(test_images, feature_extractor)\n",
        "\n",
        "# Convert labels to one-dimensional array\n",
        "test_labels_flat = np.argmax(test_labels_cat, axis=1)\n",
        "\n",
        "# Define XGBoost model parameters (with TPE)\n",
        "xgb_params = {\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': 10,\n",
        "    'learning_rate': 0.034738095185573345,\n",
        "    'max_depth': 12,\n",
        "    'min_child_weight': 2.7251833262535663,\n",
        "    'subsample': 0.3268596658325792,\n",
        "    'colsample_bytree': 0.7095410758842947,\n",
        "    'base_score': 0.24008575301562907,\n",
        "    'colsample_bylevel': 0.7719359386069632,\n",
        "    'colsample_bynode': 0.1102878470518675,\n",
        "    'max_delta_step': 0.4883774052410108,\n",
        "    'num_parallel_tree': 7,\n",
        "    'gamma': 0.005931762197591839,\n",
        "    'n_estimators': 402\n",
        "}\n",
        "\n",
        "# Train the XGBoost model with TPE parameters\n",
        "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "xgb_model.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate XGBoost model with TPE parameters\n",
        "xgb_preds = xgb_model.predict(test_features)\n",
        "xgb_accuracy = accuracy_score(test_labels_flat, xgb_preds)\n",
        "xgb_precision = precision_score(test_labels_flat, xgb_preds, average='weighted')\n",
        "xgb_recall = recall_score(test_labels_flat, xgb_preds, average='weighted')\n",
        "xgb_f1 = f1_score(test_labels_flat, xgb_preds, average='weighted')\n",
        "print(f'XGBoost with TPE Accuracy: {xgb_accuracy}')\n",
        "print(f'XGBoost with TPE Precision: {xgb_precision}')\n",
        "print(f'XGBoost with TPE Recall: {xgb_recall}')\n",
        "print(f'XGBoost with TPE F1-Score: {xgb_f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB5Or-SmM2oW",
        "outputId": "e3599c96-dc16-456e-b603-9b4df4b11ba8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
            "XGBoost with TPE Accuracy: 0.9198\n",
            "XGBoost with TPE Precision: 0.9197775962760667\n",
            "XGBoost with TPE Recall: 0.9198\n",
            "XGBoost with TPE F1-Score: 0.9197511875357878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Load dataset Fashion MNIST\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Preprocess data\n",
        "train_images = train_images.reshape(train_images.shape[0], -1)  # Flatten gambar\n",
        "test_images = test_images.reshape(test_images.shape[0], -1)  # Flatten gambar\n",
        "\n",
        "# Normalize data\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Split data train menjadi train dan validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Buat dan latih model XGBoost\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi pada data validation\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_precision = precision_score(y_val, y_val_pred, average='weighted')\n",
        "val_recall = recall_score(y_val, y_val_pred, average='weighted')\n",
        "val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
        "\n",
        "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
        "print(f'Validation Precision: {val_precision * 100:.2f}%')\n",
        "print(f'Validation Recall: {val_recall * 100:.2f}%')\n",
        "print(f'Validation F1-Score: {val_f1 * 100:.2f}%')\n",
        "\n",
        "# Prediksi pada data test\n",
        "y_test_pred = model.predict(test_images)\n",
        "test_accuracy = accuracy_score(test_labels, y_test_pred)\n",
        "test_precision = precision_score(test_labels, y_test_pred, average='weighted')\n",
        "test_recall = recall_score(test_labels, y_test_pred, average='weighted')\n",
        "test_f1 = f1_score(test_labels, y_test_pred, average='weighted')\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Test Precision: {test_precision * 100:.2f}%')\n",
        "print(f'Test Recall: {test_recall * 100:.2f}%')\n",
        "print(f'Test F1-Score: {test_f1 * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMJwiuA8e8va",
        "outputId": "79afdff2-8769-47f0-a4bd-191dc0f9a4ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [10:15:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 90.42%\n",
            "Validation Precision: 90.37%\n",
            "Validation Recall: 90.42%\n",
            "Validation F1-Score: 90.37%\n",
            "Test Accuracy: 89.26%\n",
            "Test Precision: 89.23%\n",
            "Test Recall: 89.26%\n",
            "Test F1-Score: 89.22%\n"
          ]
        }
      ]
    }
  ]
}