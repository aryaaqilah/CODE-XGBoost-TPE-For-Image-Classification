{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Check the current version of Keras\n",
        "!pip show tensorflow\n",
        "!pip show keras\n",
        "\n",
        "# Uninstall the current version of Keras\n",
        "!pip uninstall tensorflow -y\n",
        "!pip uninstall keras -y\n",
        "\n",
        "# Install the specific version of Keras\n",
        "!pip install tensorflow==2.15.1\n",
        "# !pip install keras==2.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eqa05Il1Jvka",
        "outputId": "dd986a66-d4c8-45c1-d06d-037bada16a82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.17.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine_rl, tf_keras\n",
            "Name: keras\n",
            "Version: 3.4.1\n",
            "Summary: Multi-backend Keras.\n",
            "Home-page: https://github.com/keras-team/keras\n",
            "Author: Keras team\n",
            "Author-email: keras-users@googlegroups.com\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, h5py, ml-dtypes, namex, numpy, optree, packaging, rich\n",
            "Required-by: tensorflow\n",
            "Found existing installation: tensorflow 2.17.0\n",
            "Uninstalling tensorflow-2.17.0:\n",
            "  Successfully uninstalled tensorflow-2.17.0\n",
            "Found existing installation: keras 3.4.1\n",
            "Uninstalling keras-3.4.1:\n",
            "  Successfully uninstalled keras-3.4.1\n",
            "Collecting tensorflow==2.15.1\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.15.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.1)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.1)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n",
            "Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.3.2 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "print(keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fheb4tU-J-yW",
        "outputId": "d1d58c83-b58a-49f0-f14d-18ec2d410a5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ3Ug8GeMyuj",
        "outputId": "2ce1d588-8ff5-4d1d-f64e-b8a440947143"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SO29vhuLutlU"
      },
      "outputs": [],
      "source": [
        "# !pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RRZQhWzRwicB"
      },
      "outputs": [],
      "source": [
        "# !pip install np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGik8D2HMOEv",
        "outputId": "94a55c81-40d5-4b33-8e42-86f1fa1b8d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W2KyiZyru_Mm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras import utils\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import load_model, model_from_json\n",
        "\n",
        "import optuna\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tidak dipakai"
      ],
      "metadata": {
        "id": "Crjd-gmLS4w4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOPW21GD0-JE"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# from tensorflow.keras.applications import ResNet50\n",
        "# from tensorflow.keras import layers, models, optimizers, Sequential\n",
        "# from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
        "\n",
        "# # Load and preprocess the Intel Image Dataset\n",
        "# base_dir = '/content/drive/MyDrive/intel'\n",
        "\n",
        "# train_dir = os.path.join(base_dir, 'seg_train/seg_train')\n",
        "# test_dir = os.path.join(base_dir, 'seg_test/seg_test')\n",
        "\n",
        "# # ImageDataGenerator for data augmentation and normalization\n",
        "# train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# train_generator = train_datagen.flow_from_directory(\n",
        "#     train_dir,\n",
        "#     target_size=(200, 200),\n",
        "#     batch_size=16,\n",
        "#     class_mode='categorical',\n",
        "#     subset='training'\n",
        "# )\n",
        "\n",
        "# validation_generator = train_datagen.flow_from_directory(\n",
        "#     train_dir,\n",
        "#     target_size=(200, 200),\n",
        "#     batch_size=16,\n",
        "#     class_mode='categorical',\n",
        "#     subset='validation'\n",
        "# )\n",
        "\n",
        "# test_generator = test_datagen.flow_from_directory(\n",
        "#     test_dir,\n",
        "#     target_size=(200, 200),\n",
        "#     batch_size=16,\n",
        "#     class_mode='categorical'\n",
        "# )\n",
        "\n",
        "# conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
        "\n",
        "# # Freeze the convolutional base\n",
        "# conv_base.trainable = False\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(conv_base)\n",
        "# model.add(GlobalAveragePooling2D())\n",
        "# model.add(Dense(512, activation='relu'))\n",
        "# model.add(Dropout(0.50))\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dropout(0.50))\n",
        "# model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "# # Use Adam optimizer for faster convergence\n",
        "# model.compile(\n",
        "#     optimizer=optimizers.Adam(learning_rate=2e-4),\n",
        "#     loss='categorical_crossentropy',\n",
        "#     metrics=['acc']\n",
        "# )\n",
        "\n",
        "# print('Build and save ResNet-50 model')\n",
        "\n",
        "# history = model.fit(\n",
        "#     train_generator,\n",
        "#     epochs=50,\n",
        "#     validation_data=validation_generator,\n",
        "#     steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "#     validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        "# )\n",
        "\n",
        "# model_json = model.to_json()\n",
        "# with open('model_resnet.json', 'w') as json_file:\n",
        "#     json_file.write(model_json)\n",
        "\n",
        "# model.save_weights('model_resnet.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "seg_train = '/content/drive/MyDrive/intel/seg_train/seg_train'\n",
        "seg_test = '/content/drive/MyDrive/intel/seg_test/seg_test/'\n",
        "seg_pred = '/content/drive/MyDrive/intel/seg_pred/seg_pred/'\n",
        "\n",
        "generate = ImageDataGenerator(rescale=1./255,\n",
        "                              shear_range=0.2,\n",
        "                              zoom_range=0.2,\n",
        "                              horizontal_flip=True)\n",
        "training_set = generate.flow_from_directory(seg_train,\n",
        "                                            target_size=(100, 100),\n",
        "                                            batch_size=256,  # Mengurangi batch size\n",
        "                                            classes=[\"buildings\",\"forest\",\"glacier\",\"mountain\",\"sea\",\"street\"],\n",
        "                                            class_mode='categorical')\n",
        "test_set = generate.flow_from_directory(seg_test,\n",
        "                                        target_size=(100, 100),\n",
        "                                        batch_size=256,  # Mengurangi batch size\n",
        "                                        classes=[\"buildings\",\"forest\",\"glacier\",\"mountain\",\"sea\",\"street\"],\n",
        "                                        class_mode='categorical')\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(100, 100, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))  # Mengurangi jumlah neuron\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks untuk mengurangi waktu pelatihan\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(training_set, epochs=15, verbose=1, validation_data=test_set, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# Evaluate the model\n",
        "score = model.evaluate(test_set, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(test_set)\n",
        "\n",
        "# Convert predictions and true labels to class indices\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = test_set.classes\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true_classes, y_pred_classes, target_names=[\"buildings\",\"forest\",\"glacier\",\"mountain\",\"sea\",\"street\"])\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnG9616htxtn",
        "outputId": "e097b7ad-064b-49a4-9781-73c1aa7edef4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14044 images belonging to 6 classes.\n",
            "Found 3000 images belonging to 6 classes.\n",
            "Epoch 1/15\n",
            "55/55 [==============================] - 4493s 82s/step - loss: 1.2350 - accuracy: 0.4989 - val_loss: 0.9839 - val_accuracy: 0.6133 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "55/55 [==============================] - 108s 2s/step - loss: 0.9199 - accuracy: 0.6465 - val_loss: 0.8400 - val_accuracy: 0.6823 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "55/55 [==============================] - 104s 2s/step - loss: 0.8157 - accuracy: 0.7001 - val_loss: 0.7970 - val_accuracy: 0.6930 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "55/55 [==============================] - 106s 2s/step - loss: 0.7292 - accuracy: 0.7368 - val_loss: 0.6763 - val_accuracy: 0.7457 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "55/55 [==============================] - 103s 2s/step - loss: 0.6748 - accuracy: 0.7559 - val_loss: 0.6240 - val_accuracy: 0.7727 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "55/55 [==============================] - 106s 2s/step - loss: 0.6011 - accuracy: 0.7859 - val_loss: 0.7051 - val_accuracy: 0.7460 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "55/55 [==============================] - 104s 2s/step - loss: 0.5937 - accuracy: 0.7865 - val_loss: 0.5646 - val_accuracy: 0.7957 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "55/55 [==============================] - 104s 2s/step - loss: 0.5637 - accuracy: 0.7993 - val_loss: 0.5271 - val_accuracy: 0.8093 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "55/55 [==============================] - 104s 2s/step - loss: 0.5094 - accuracy: 0.8232 - val_loss: 0.4990 - val_accuracy: 0.8237 - lr: 0.0010\n",
            "Epoch 10/15\n",
            "55/55 [==============================] - 103s 2s/step - loss: 0.4943 - accuracy: 0.8248 - val_loss: 0.5177 - val_accuracy: 0.8190 - lr: 0.0010\n",
            "Epoch 11/15\n",
            "55/55 [==============================] - 105s 2s/step - loss: 0.4841 - accuracy: 0.8283 - val_loss: 0.5488 - val_accuracy: 0.7967 - lr: 0.0010\n",
            "Epoch 12/15\n",
            "55/55 [==============================] - 104s 2s/step - loss: 0.4639 - accuracy: 0.8354 - val_loss: 0.4518 - val_accuracy: 0.8423 - lr: 0.0010\n",
            "Epoch 13/15\n",
            "55/55 [==============================] - 109s 2s/step - loss: 0.4800 - accuracy: 0.8290 - val_loss: 0.4578 - val_accuracy: 0.8430 - lr: 0.0010\n",
            "Epoch 14/15\n",
            "55/55 [==============================] - 107s 2s/step - loss: 0.4174 - accuracy: 0.8520 - val_loss: 0.4284 - val_accuracy: 0.8547 - lr: 0.0010\n",
            "Epoch 15/15\n",
            "55/55 [==============================] - 107s 2s/step - loss: 0.4057 - accuracy: 0.8552 - val_loss: 0.4253 - val_accuracy: 0.8500 - lr: 0.0010\n",
            "12/12 [==============================] - 18s 2s/step - loss: 0.4367 - accuracy: 0.8470\n",
            "Test loss: 0.4366558790206909\n",
            "Test accuracy: 0.847000002861023\n",
            "12/12 [==============================] - 19s 2s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   buildings       0.16      0.20      0.18       437\n",
            "      forest       0.13      0.13      0.13       474\n",
            "     glacier       0.19      0.16      0.18       553\n",
            "    mountain       0.19      0.21      0.20       525\n",
            "         sea       0.18      0.17      0.17       510\n",
            "      street       0.18      0.17      0.17       501\n",
            "\n",
            "    accuracy                           0.17      3000\n",
            "   macro avg       0.17      0.17      0.17      3000\n",
            "weighted avg       0.17      0.17      0.17      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Memuat dataset CIFAR-10\n",
        "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# train_size = int(1 * x_train.shape[0])\n",
        "# indices = np.random.permutation(x_train.shape[0])\n",
        "# x_train = x_train[indices][:train_size]\n",
        "# y_train = y_train[indices][:train_size]\n",
        "\n",
        "# x_train = x_train / 255.0\n",
        "# x_test = x_test / 255.0\n",
        "\n",
        "# y_train = to_categorical(y_train, 10)\n",
        "# y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# print('CIFAR-10 dataset loaded')\n",
        "\n",
        "# conv_base = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
        "# model = models.Sequential()\n",
        "# model.add(layers.UpSampling2D((2,2), input_shape=(32, 32, 3)))\n",
        "# model.add(layers.UpSampling2D((2,2)))\n",
        "# model.add(layers.Resizing(200, 200))\n",
        "# model.add(conv_base)\n",
        "# model.add(layers.Flatten(name='flatten'))  # Menambahkan nama ke layer Flatten\n",
        "# model.add(layers.BatchNormalization())\n",
        "# model.add(layers.Dense(128, activation='relu'))\n",
        "# model.add(layers.Dropout(0.5))\n",
        "# model.add(layers.BatchNormalization())\n",
        "# model.add(layers.Dense(64, activation='relu'))\n",
        "# model.add(layers.Dropout(0.5))\n",
        "# model.add(layers.BatchNormalization())\n",
        "# model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "#     loss='binary_crossentropy',\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# print('Build and save ResNet-50 model')\n",
        "\n",
        "# history = model.fit(x_train, y_train,\n",
        "#     epochs=5,\n",
        "#     batch_size=20,\n",
        "#     validation_data=(x_test, y_test))\n",
        "\n",
        "# # Menyimpan arsitektur model ke file JSON\n",
        "# model_json = model.to_json()\n",
        "# with open('model_resnet.json', 'w') as json_file:\n",
        "#     json_file.write(model_json)\n",
        "\n",
        "# # Menyimpan bobot model ke file HDF5 dengan ekstensi yang benar\n",
        "# model.save_weights('model_resnet.weights.h5')"
      ],
      "metadata": {
        "id": "N9smJ56eWCRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "with open('model_resnet.json', 'w') as json_file:\n",
        "\tjson_file.write(model_json)\n",
        "model.save_weights('model_resnet.h5')"
      ],
      "metadata": {
        "id": "5XPU1D2yeUUv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1F3iacUHT5a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSgfkun7mgs1"
      },
      "outputs": [],
      "source": [
        "# # Evaluate the model on the test data and calculate additional metrics\n",
        "# test_steps = test_generator.samples // test_generator.batch_size\n",
        "# test_generator.reset()\n",
        "# predictions = model.predict(test_generator, steps=test_steps)\n",
        "# y_pred = np.argmax(predictions, axis=1)\n",
        "# y_true = test_generator.classes[:len(y_pred)]\n",
        "\n",
        "# # Calculate metrics\n",
        "# accuracy = accuracy_score(y_true, y_pred)\n",
        "# precision = precision_score(y_true, y_pred, average='weighted')\n",
        "# recall = recall_score(y_true, y_pred, average='weighted')\n",
        "# f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "# print(f'Accuracy: {accuracy:.4f}')\n",
        "# print(f'Precision: {precision:.4f}')\n",
        "# print(f'Recall: {recall:.4f}')\n",
        "# print(f'F1 Score: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import model_from_json\n",
        "# from tensorflow.keras import optimizers\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# import xgboost as xgb\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# import pickle\n",
        "\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# def load_cnn_model(X_test, y_test):\n",
        "#     json_file = open('model_resnet.json', 'r')\n",
        "#     loaded_model_json = json_file.read()\n",
        "#     json_file.close()\n",
        "#     model = model_from_json(loaded_model_json)\n",
        "#     model.load_weights('model_resnet.h5')  # load weights into new model\n",
        "\n",
        "#     model.compile(\n",
        "#         optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "#         loss='categorical_crossentropy',\n",
        "#         metrics=['accuracy']\n",
        "#     )\n",
        "\n",
        "#     return model\n",
        "\n",
        "# def get_feature_layer(model, data):\n",
        "#     total_layers = len(model.layers)\n",
        "#     fl_index = total_layers - 7  # Get the correct index for the feature layer\n",
        "\n",
        "#     feature_layer_model = tf.keras.Model(\n",
        "#         inputs=model.input,\n",
        "#         outputs=model.get_layer(index=fl_index).output\n",
        "#     )\n",
        "#     feature_layer_output = feature_layer_model.predict(data)\n",
        "\n",
        "#     return feature_layer_output\n",
        "\n",
        "# def xgb_model(X_train, y_train, X_test, y_test):\n",
        "#     y_train = y_train.flatten()\n",
        "#     y_test = y_test.flatten()\n",
        "\n",
        "#     dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "#     dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "#     params = {\n",
        "#         'max_depth': 12,\n",
        "#         'eta': 0.05,\n",
        "#         'objective': 'multi:softprob',\n",
        "#         'num_class': 6,\n",
        "#         'early_stopping_rounds': 5,\n",
        "#         'eval_metric': 'merror'\n",
        "#     }\n",
        "\n",
        "#     watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
        "#     n_rounds = 150\n",
        "\n",
        "#     model = xgb.train(params, dtrain, n_rounds, evals=watchlist)\n",
        "\n",
        "#     return model\n",
        "\n",
        "# def evaluate_model(model, X_test, y_test):\n",
        "#     dtest = xgb.DMatrix(X_test)\n",
        "#     y_pred_prob = model.predict(dtest)\n",
        "#     y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "#     precision = precision_score(y_test, y_pred, average='macro')\n",
        "#     recall = recall_score(y_test, y_pred, average='macro')\n",
        "#     f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "#     print(f'Accuracy: {accuracy}')\n",
        "#     print(f'Precision: {precision}')\n",
        "#     print(f'Recall: {recall}')\n",
        "#     print(f'F1 Score: {f1}')\n",
        "\n",
        "#     return accuracy, precision, recall, f1\n",
        "\n",
        "# def load_intel_image_dataset(base_dir, img_size):\n",
        "#     datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
        "\n",
        "#     train_generator = datagen.flow_from_directory(\n",
        "#         base_dir,\n",
        "#         target_size=(img_size, img_size),\n",
        "#         batch_size=32,\n",
        "#         class_mode='categorical',\n",
        "#         subset='training'\n",
        "#     )\n",
        "\n",
        "#     validation_generator = datagen.flow_from_directory(\n",
        "#         base_dir,\n",
        "#         target_size=(img_size, img_size),\n",
        "#         batch_size=32,\n",
        "#         class_mode='categorical',\n",
        "#         subset='validation'\n",
        "#     )\n",
        "\n",
        "#     X_train, y_train = next(train_generator)\n",
        "#     for i in range(1, len(train_generator)):\n",
        "#         x, y = next(train_generator)\n",
        "#         X_train = np.concatenate((X_train, x))\n",
        "#         y_train = np.concatenate((y_train, y))\n",
        "\n",
        "#     X_test, y_test = next(validation_generator)\n",
        "#     for i in range(1, len(validation_generator)):\n",
        "#         x, y = next(validation_generator)\n",
        "#         X_test = np.concatenate((X_test, x))\n",
        "#         y_test = np.concatenate((y_test, y))\n",
        "\n",
        "#     return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "# def main():\n",
        "#     # Load Intel Image Dataset\n",
        "#     base_dir = '/content/drive/MyDrive/intel'\n",
        "#     img_size = 150\n",
        "#     (X_train, y_train), (X_test, y_test) = load_intel_image_dataset(base_dir, img_size)\n",
        "\n",
        "#     # Flatten the labels\n",
        "#     y_train = np.argmax(y_train, axis=1)\n",
        "#     y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "#     # Load the CNN model and extract features\n",
        "#     cnn_model = load_cnn_model(X_test, y_test)\n",
        "\n",
        "#     X_train_cnn = get_feature_layer(cnn_model, X_train)  # Extract features\n",
        "#     X_test_cnn = get_feature_layer(cnn_model, X_test)  # Extract features\n",
        "\n",
        "#     # Train XGBoost model with correct objective function and label format\n",
        "#     model = xgb_model(X_train_cnn, y_train, X_test_cnn, y_test)\n",
        "\n",
        "#     print(\"Build and save of XGBoost Model\")\n",
        "#     pickle.dump(model, open(\"cnn_xgboost_resnet_final.pickle.dat\", \"wb\"))\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     evaluate_model(model, X_test_cnn, y_test)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "hjYb8uwAfBZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import model_from_json\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "\n",
        "def load_cnn_model():\n",
        "    # Load model architecture\n",
        "    with open('model_resnet.json', 'r') as json_file:\n",
        "        loaded_model_json = json_file.read()\n",
        "\n",
        "    # Load model weights\n",
        "    model = model_from_json(loaded_model_json)\n",
        "    model.load_weights('model_resnet.h5')\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_feature_layer(model, data):\n",
        "    # Get the feature extraction layer\n",
        "    feature_layer_model = tf.keras.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=model.layers[-7].output\n",
        "    )\n",
        "\n",
        "    # Extract features\n",
        "    feature_layer_output = feature_layer_model.predict(data)\n",
        "    return feature_layer_output\n",
        "\n",
        "def xgb_model(X_train, y_train, X_test, y_test):\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "    params = {\n",
        "        'max_depth': 12,\n",
        "        'eta': 0.05,\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': 6,\n",
        "        'eval_metric': 'merror'\n",
        "    }\n",
        "\n",
        "    model = xgb.train(params, dtrain, num_boost_round=150, evals=[(dtrain, 'train'), (dtest, 'eval')])\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "    y_pred_prob = model.predict(dtest)\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='macro')\n",
        "    recall = recall_score(y_test, y_pred, average='macro')\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'F1 Score: {f1}')\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def load_intel_image_dataset(base_dir, img_size):\n",
        "    datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
        "\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        base_dir,\n",
        "        target_size=(img_size, img_size),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='training'\n",
        "    )\n",
        "\n",
        "    validation_generator = datagen.flow_from_directory(\n",
        "        base_dir,\n",
        "        target_size=(img_size, img_size),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='validation'\n",
        "    )\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: train_generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None, train_generator.num_classes), dtype=tf.float32)\n",
        "        )\n",
        "    ).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    validation_dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: validation_generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, img_size, img_size, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None, validation_generator.num_classes), dtype=tf.float32)\n",
        "        )\n",
        "    ).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    X_train, y_train = [], []\n",
        "    for x, y in train_dataset:\n",
        "        X_train.append(x)\n",
        "        y_train.append(y)\n",
        "\n",
        "    X_train = np.concatenate(X_train)\n",
        "    y_train = np.concatenate(y_train)\n",
        "\n",
        "    X_test, y_test = [], []\n",
        "    for x, y in validation_dataset:\n",
        "        X_test.append(x)\n",
        "        y_test.append(y)\n",
        "\n",
        "    X_test = np.concatenate(X_test)\n",
        "    y_test = np.concatenate(y_test)\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "def main():\n",
        "    # Load Intel Image Dataset\n",
        "    base_dir = '/content/drive/MyDrive/intel/'\n",
        "    img_size = 150\n",
        "    (X_train, y_train), (X_test, y_test) = load_intel_image_dataset(base_dir, img_size)\n",
        "\n",
        "    # Convert labels from one-hot to single dimension\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Load the CNN model\n",
        "    cnn_model = load_cnn_model()\n",
        "\n",
        "    # Extract features using the CNN model\n",
        "    X_train_cnn = get_feature_layer(cnn_model, X_train)\n",
        "    X_test_cnn = get_feature_layer(cnn_model, X_test)\n",
        "\n",
        "    # Train XGBoost model\n",
        "    xgb_model_trained = xgb_model(X_train_cnn, y_train, X_test_cnn, y_test)\n",
        "\n",
        "    # Save the XGBoost model\n",
        "    with open(\"cnn_xgboost_resnet_final.pickle.dat\", \"wb\") as f:\n",
        "        pickle.dump(xgb_model_trained, f)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(xgb_model_trained, X_test_cnn, y_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "8_Lrbo5KkP3h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "db1ff5ab-8f63-4051-82e2-bb9849ae0b5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 19513 images belonging to 3 classes.\n",
            "Found 4877 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9b292308f5cc>\u001b[0m in \u001b[0;36m<cell line: 151>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-9b292308f5cc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/intel/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_intel_image_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m# Convert labels from one-hot to single dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9b292308f5cc>\u001b[0m in \u001b[0;36mload_intel_image_dataset\u001b[0;34m(base_dir, img_size)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    774\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3022\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3025\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m         \"output_shapes\", output_shapes)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from keras.models import model_from_json\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from hyperopt.pyll.base import scope\n",
        "\n",
        "feature_extractor = Model(inputs=model.input, outputs=model.layers[-3].output)\n",
        "train_features = feature_extractor.predict(training_set)\n",
        "test_features = feature_extractor.predict(test_set)\n",
        "\n",
        "# Get labels\n",
        "train_labels = training_set.classes\n",
        "test_labels = test_set.classes\n",
        "\n",
        "# Split data for hyperparameter tuning\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define objective function for TPE\n",
        "def objective(params):\n",
        "    model = XGBClassifier(\n",
        "        **{k: int(v) if k in ['max_depth', 'n_estimators', 'min_child_weight', 'num_parallel_tree'] else v for k, v in params.items()},\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    accuracy = np.mean(y_pred == y_val)\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
        "\n",
        "# Define hyperparameter space\n",
        "space = {\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
        "    'max_depth': hp.quniform('max_depth', 1, 15, 1),\n",
        "    'min_child_weight': hp.uniform('min_child_weight', 0, 5),\n",
        "    'subsample': hp.uniform('subsample', 0.1, 1),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1),\n",
        "    'base_score': hp.uniform('base_score', 0.1, 0.9),\n",
        "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1),\n",
        "    'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1),\n",
        "    'max_delta_step': hp.uniform('max_delta_step', 0, 1),\n",
        "    'num_parallel_tree': hp.quniform('num_parallel_tree', 1, 10, 1),\n",
        "    'gamma': hp.uniform('gamma', 0, 0.1)\n",
        "}\n",
        "\n",
        "# Run TPE optimization\n",
        "trials = Trials()\n",
        "best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
        "\n",
        "# Convert best_params to proper types\n",
        "best_params = {k: int(v) if k in ['max_depth', 'n_estimators', 'min_child_weight', 'num_parallel_tree'] else v for k, v in best_params.items()}\n",
        "\n",
        "# Train final model with best parameters\n",
        "final_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
        "final_model.fit(train_features, train_labels)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = final_model.predict(test_features)\n",
        "accuracy = np.mean(y_pred == test_labels)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(test_labels, y_pred, target_names=[\"buildings\",\"forest\",\"glacier\",\"mountain\",\"sea\",\"street\"])\n",
        "print(report)\n",
        "\n",
        "# Save the final model\n",
        "final_model.save_model('xgboost_model.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgZ56lQP5hVY",
        "outputId": "1e324b89-a520-452f-e7d8-5d6887c62240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28/28 [==============================] - 64s 2s/step\n",
            "6/6 [==============================] - 14s 2s/step\n",
            "  0%|          | 0/10 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [04:26:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 10%|█         | 1/10 [02:29<22:29, 149.92s/trial, best loss: -0.16411534353862584]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [04:29:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 20%|██        | 2/10 [08:57<38:36, 289.57s/trial, best loss: -0.18120327518689924]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [04:35:30] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 30%|███       | 3/10 [12:41<30:18, 259.77s/trial, best loss: -0.18120327518689924]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [04:39:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 40%|████      | 4/10 [22:11<38:13, 382.21s/trial, best loss: -0.18120327518689924]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [04:48:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 50%|█████     | 5/10 [22:40<21:14, 254.84s/trial, best loss: -0.18120327518689924]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [04:49:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 60%|██████    | 6/10 [31:33<23:17, 349.37s/trial, best loss: -0.18120327518689924]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [04:58:06] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 70%|███████   | 7/10 [32:01<12:13, 244.35s/trial, best loss: -0.18120327518689924]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [04:58:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 80%|████████  | 8/10 [33:58<06:47, 203.90s/trial, best loss: -0.18120327518689924]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [05:00:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 90%|█████████ | 9/10 [34:15<02:25, 145.45s/trial, best loss: -0.18120327518689924]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [05:00:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 10/10 [34:34<00:00, 207.49s/trial, best loss: -0.18120327518689924]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [05:01:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.161\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   buildings       0.16      0.13      0.14       437\n",
            "      forest       0.16      0.14      0.15       474\n",
            "     glacier       0.16      0.16      0.16       553\n",
            "    mountain       0.17      0.23      0.20       525\n",
            "         sea       0.16      0.15      0.15       510\n",
            "      street       0.15      0.15      0.15       501\n",
            "\n",
            "    accuracy                           0.16      3000\n",
            "   macro avg       0.16      0.16      0.16      3000\n",
            "weighted avg       0.16      0.16      0.16      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params['max_depth'] = int(best_params['max_depth'])\n",
        "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
        "best_params['min_child_weight'] = int(best_params['min_child_weight'])\n",
        "\n",
        "final_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
        "final_model.fit(train_features, train_labels)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = final_model.predict(test_features)\n",
        "accuracy = np.mean(y_pred == test_labels)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(test_labels, y_pred, target_names=[\"buildings\",\"forest\",\"glacier\",\"mountain\",\"sea\",\"street\"])\n",
        "print(report)\n",
        "\n",
        "# Save the final model\n",
        "final_model.save_model('xgboost_model.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xdsis-9_R9C",
        "outputId": "492bca91-f074-4aa9-ea09-c9ca82e9ef7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:58:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.17533333333333334\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   buildings       0.15      0.14      0.15       437\n",
            "      forest       0.17      0.15      0.16       474\n",
            "     glacier       0.17      0.17      0.17       553\n",
            "    mountain       0.20      0.25      0.22       525\n",
            "         sea       0.16      0.15      0.16       510\n",
            "      street       0.18      0.18      0.18       501\n",
            "\n",
            "    accuracy                           0.18      3000\n",
            "   macro avg       0.17      0.17      0.17      3000\n",
            "weighted avg       0.17      0.18      0.17      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzw3_oSBshjK"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import xgboost as xgb\n",
        "# from keras.models import model_from_json\n",
        "# from keras import optimizers\n",
        "# from keras.datasets import cifar10\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# import pickle\n",
        "\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# def load_cnn_model():\n",
        "#     json_file = open('model_resnet.json', 'r')\n",
        "#     loaded_model_json = json_file.read()\n",
        "#     json_file.close()\n",
        "#     model = model_from_json(loaded_model_json)\n",
        "#     # model = model_from_json(loaded_model_json)\n",
        "#     model.load_weights('model_resnet.weights.h5')\n",
        "#     model.compile(\n",
        "#         # optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "#         loss='categorical_crossentropy',\n",
        "#         metrics=['accuracy']\n",
        "#     )\n",
        "\n",
        "#     return model\n",
        "\n",
        "# def get_feature_layer(model, data):\n",
        "#     # Access the feature layer by name instead of index\n",
        "#     feature_layer_model = keras.Model(\n",
        "#         inputs=model.input,\n",
        "#         outputs=model.get_layer(name='flatten').output # Change here\n",
        "#     )\n",
        "#     feature_layer_output = feature_layer_model.predict(data)\n",
        "\n",
        "#     return feature_layer_output\n",
        "\n",
        "# def xgb_model(X_train, y_train, X_test, y_test):\n",
        "\n",
        "#     y_train = y_train.flatten()\n",
        "#     y_test = y_test.flatten()\n",
        "\n",
        "#     dtrain = xgb.DMatrix(\n",
        "#         X_train,\n",
        "#         label=y_train\n",
        "#     )\n",
        "\n",
        "#     dtest = xgb.DMatrix(\n",
        "#         X_test,\n",
        "#         label=y_test\n",
        "#     )\n",
        "\n",
        "#     params = {\n",
        "#         # 'max_depth': 12,\n",
        "#         # 'eta': 0.05,\n",
        "#         'objective': 'multi:softprob',\n",
        "#         'num_class': 10,\n",
        "#         'early_stopping_rounds': 5,\n",
        "#         'eval_metric': 'merror'\n",
        "#     }\n",
        "\n",
        "#     watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
        "#     # n_rounds = 100\n",
        "\n",
        "#     model = xgb.train(\n",
        "#         params,\n",
        "#         dtrain,\n",
        "#         # n_rounds,\n",
        "#         evals=watchlist\n",
        "#     )\n",
        "\n",
        "#     return model\n",
        "\n",
        "# def evaluate_model(model, X_test, y_test):\n",
        "#     dtest = xgb.DMatrix(X_test)\n",
        "#     y_pred_prob = model.predict(dtest)\n",
        "#     y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "#     precision = precision_score(y_test, y_pred, average='macro')\n",
        "#     recall = recall_score(y_test, y_pred, average='macro')\n",
        "#     f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "#     print(f'Accuracy: {accuracy}')\n",
        "#     print(f'Precision: {precision}')\n",
        "#     print(f'Recall: {recall}')\n",
        "#     print(f'F1 Score: {f1}')\n",
        "\n",
        "#     return accuracy, precision, recall, f1\n",
        "\n",
        "# def main():\n",
        "#     # Load CIFAR-10 dataset\n",
        "#     (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#     indices = np.random.permutation(X_train.shape[0])\n",
        "#     train_size = 40000  # Ubah sesuai kebutuhan\n",
        "#     X_train = X_train[indices][:train_size]\n",
        "#     y_train = y_train[indices][:train_size]\n",
        "\n",
        "#     # Normalize the data\n",
        "#     X_train = X_train.astype('float32') / 255.0\n",
        "#     X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "#     y_train = y_train.flatten()\n",
        "#     y_test = y_test.flatten()\n",
        "\n",
        "#     # Load the CNN model and extract features\n",
        "#     cnn_model = load_cnn_model()\n",
        "\n",
        "#     X_train_cnn = get_feature_layer(cnn_model, X_train)\n",
        "#     X_test_cnn = get_feature_layer(cnn_model, X_test)\n",
        "\n",
        "#     # Train XGBoost model\n",
        "#     model = xgb_model(X_train_cnn, y_train, X_test_cnn, y_test)\n",
        "\n",
        "#     print(\"Build and save of XGBoost Model\")\n",
        "#     pickle.dump(model, open(\"cnn_xgboost_resnet_final.pickle.dat\", \"wb\"))\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     evaluate_model(model, X_test_cnn, y_test)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9j5z0rNUkPq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from xgboost.callback import TrainingCallback\n",
        "import keras\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "def load_cnn_model():\n",
        "    json_file = open('model_resnet.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    model = model_from_json(loaded_model_json)\n",
        "    model.load_weights('model_resnet.h5')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_feature_layer(model, data):\n",
        "    total_layers = len(model.layers)\n",
        "    fl_index = total_layers - 7  # Get the correct index for the feature layer\n",
        "\n",
        "    feature_layer_model = keras.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=model.get_layer(index=fl_index).output\n",
        "    )\n",
        "    feature_layer_output = feature_layer_model.predict(data)\n",
        "\n",
        "    return feature_layer_output\n",
        "\n",
        "def objective(space, X_train_cnn, y_train, X_valid_cnn, y_valid):\n",
        "    params = {\n",
        "        'learning_rate': space['learning_rate'],\n",
        "        'max_depth': int(space['max_depth']),\n",
        "        'min_child_weight': space['min_child_weight'],\n",
        "        'subsample': space['subsample'],\n",
        "        'colsample_bytree': space['colsample_bytree'],\n",
        "        'base_score': space['base_score'],\n",
        "        'colsample_bylevel': space['colsample_bylevel'],\n",
        "        'colsample_bynode': space['colsample_bynode'],\n",
        "        'num_parallel_tree': int(space['num_parallel_tree']),\n",
        "        'max_delta_step': space['max_delta_step'],\n",
        "        'gamma': space['gamma'],\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': 10,\n",
        "        'eval_metric': 'merror'\n",
        "    }\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train_cnn, label=y_train)\n",
        "    dvalid = xgb.DMatrix(X_valid_cnn, label=y_valid)\n",
        "\n",
        "    evals_result = {}\n",
        "    model = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dvalid, 'eval')],\n",
        "                      early_stopping_rounds=10, evals_result=evals_result, verbose_eval=False)\n",
        "\n",
        "    pred = model.predict(dvalid)\n",
        "    pred_labels = np.argmax(pred, axis=1)\n",
        "    accuracy = accuracy_score(y_valid, pred_labels)\n",
        "\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK}\n",
        "\n",
        "def evaluate_model(model, X_test_cnn, y_test):\n",
        "    dtest = xgb.DMatrix(X_test_cnn)\n",
        "    pred = model.predict(dtest)\n",
        "    pred_labels = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, pred_labels)\n",
        "    precision = precision_score(y_test, pred_labels, average='weighted')\n",
        "    recall = recall_score(y_test, pred_labels, average='weighted')\n",
        "    f1 = f1_score(y_test, pred_labels, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "\n",
        "class EarlyStopCallback(TrainingCallback):\n",
        "    def __init__(self, threshold):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def after_iteration(self, model, epoch, evals_log):\n",
        "        for eval_name, eval_metrics in evals_log.items():\n",
        "            for metric_name, metric_values in eval_metrics.items():\n",
        "                if eval_name == 'eval' and metric_name == 'merror' and 1 - metric_values[-1] > self.threshold:\n",
        "                    print(f\"Stopping early. Accuracy reached {1 - metric_values[-1]}\")\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    # Load CIFAR-10 dataset\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    # Preprocess data: normalize images\n",
        "    X_train = X_train.astype('float32') / 255.0\n",
        "    X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "    # Data Augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.2,\n",
        "        shear_range=0.2\n",
        "    )\n",
        "\n",
        "    datagen.fit(X_train)\n",
        "\n",
        "    # Convert labels to single-dimensional integers\n",
        "    y_train = y_train.flatten()\n",
        "    y_test = y_test.flatten()\n",
        "\n",
        "    # Split training data into training and validation sets\n",
        "    train_size = int(0.8 * len(X_train))\n",
        "    X_train, X_valid = X_train[:train_size], X_train[train_size:]\n",
        "    y_train, y_valid = y_train[:train_size], y_train[train_size:]\n",
        "\n",
        "    # Load the CNN model and extract features\n",
        "    cnn_model = load_cnn_model()\n",
        "\n",
        "    X_train_cnn = get_feature_layer(cnn_model, X_train)\n",
        "    X_valid_cnn = get_feature_layer(cnn_model, X_valid)\n",
        "    X_test_cnn = get_feature_layer(cnn_model, X_test)\n",
        "\n",
        "    # Define the search space\n",
        "    space = {\n",
        "        'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),\n",
        "        'max_depth': hp.quniform('max_depth', 1, 15, 1),\n",
        "        'min_child_weight': hp.uniform('min_child_weight', 0, 5),\n",
        "        'subsample': hp.uniform('subsample', 0.1, 1),\n",
        "        'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1),\n",
        "        'base_score': hp.uniform('base_score', 0.1, 0.9),\n",
        "        'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1),\n",
        "        'colsample_bynode': hp.uniform('colsample_bynode', 0.1, 1),\n",
        "        'max_delta_step': hp.uniform('max_delta_step', 0, 1),\n",
        "        'num_parallel_tree': hp.quniform('num_parallel_tree', 1, 10, 1),\n",
        "        'gamma': hp.uniform('gamma', 0, 0.1)\n",
        "    }\n",
        "\n",
        "    # Run the optimization\n",
        "    trials = Trials()\n",
        "    best = fmin(\n",
        "        fn=lambda space: objective(space, X_train_cnn, y_train, X_valid_cnn, y_valid),\n",
        "        space=space,\n",
        "        algo=tpe.suggest,\n",
        "        max_evals=100,\n",
        "        trials=trials\n",
        "    )\n",
        "\n",
        "    print(\"Best hyperparameters found were:\", best)\n",
        "\n",
        "    # Train the final model with the best hyperparameters\n",
        "    best_params = {\n",
        "        'learning_rate': best['learning_rate'],\n",
        "        'max_depth': int(best['max_depth']),\n",
        "        'min_child_weight': best['min_child_weight'],\n",
        "        'subsample': best['subsample'],\n",
        "        'colsample_bytree': best['colsample_bytree'],\n",
        "        'base_score': best['base_score'],\n",
        "        'colsample_bylevel': best['colsample_bylevel'],\n",
        "        'colsample_bynode': best['colsample_bynode'],\n",
        "        'num_parallel_tree': int(best['num_parallel_tree']),\n",
        "        'max_delta_step': best['max_delta_step'],\n",
        "        'gamma': best['gamma'],\n",
        "        'objective': 'multi:softprob',\n",
        "        'num_class': 10,\n",
        "        'eval_metric': 'merror',\n",
        "        'booster': 'gbtree'\n",
        "    }\n",
        "\n",
        "    # Combine training and validation data\n",
        "    X_combined_cnn = np.vstack((X_train_cnn, X_valid_cnn))\n",
        "    y_combined = np.hstack((y_train, y_valid))\n",
        "\n",
        "    dtrain_combined = xgb.DMatrix(X_combined_cnn, label=y_combined)\n",
        "\n",
        "    # Define custom callback\n",
        "    early_stop_callback = EarlyStopCallback(threshold=0.948)\n",
        "\n",
        "    # Train the final model\n",
        "    final_model = xgb.train(best_params, dtrain_combined, num_boost_round=1000, callbacks=[early_stop_callback])\n",
        "\n",
        "    # Save the final model\n",
        "    final_model.save_model('optimized_xgboost_model.dat')\n",
        "\n",
        "    # Evaluate the final model\n",
        "    evaluate_model(final_model, X_test_cnn, y_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG4or2E5GtfK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2LExrlTNe-gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-v-E3xSb2X1"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import pickle\n",
        "# import xgboost as xgb\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from tensorflow.keras.datasets import cifar10\n",
        "# from tensorflow.keras.models import model_from_json\n",
        "# from tensorflow.keras import optimizers\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# import keras\n",
        "\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# def load_cnn_model():\n",
        "#     json_file = open('model_resnet.json', 'r')\n",
        "#     loaded_model_json = json_file.read()\n",
        "#     json_file.close()\n",
        "#     model = model_from_json(loaded_model_json)\n",
        "#     model.load_weights('model_resnet.h5')  # load weights into new model\n",
        "\n",
        "#     model.compile(\n",
        "#         optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
        "#         loss='categorical_crossentropy',\n",
        "#         metrics=['accuracy']\n",
        "#     )\n",
        "\n",
        "#     return model\n",
        "\n",
        "# def get_feature_layer(model, data):\n",
        "#     total_layers = len(model.layers)\n",
        "#     fl_index = total_layers - 7  # Get the correct index for the feature layer\n",
        "\n",
        "#     feature_layer_model = keras.Model(\n",
        "#         inputs=model.input,\n",
        "#         outputs=model.get_layer(index=fl_index).output\n",
        "#     )\n",
        "#     feature_layer_output = feature_layer_model.predict(data)\n",
        "\n",
        "#     return feature_layer_output\n",
        "\n",
        "# def evaluate_model(model, X_test_cnn, y_test):\n",
        "#     dtest = xgb.DMatrix(X_test_cnn)\n",
        "#     pred = model.predict(dtest)\n",
        "#     pred_labels = np.argmax(pred, axis=1)\n",
        "\n",
        "#     accuracy = accuracy_score(y_test, pred_labels)\n",
        "#     precision = precision_score(y_test, pred_labels, average='weighted')\n",
        "#     recall = recall_score(y_test, pred_labels, average='weighted')\n",
        "#     f1 = f1_score(y_test, pred_labels, average='weighted')\n",
        "\n",
        "#     print(f\"Accuracy: {accuracy}\")\n",
        "#     print(f\"Precision: {precision}\")\n",
        "#     print(f\"Recall: {recall}\")\n",
        "#     print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# def main():\n",
        "#     # Load CIFAR-10 dataset\n",
        "#     (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#     # Preprocess data: normalize images\n",
        "#     X_train = X_train.astype('float32') / 255.0\n",
        "#     X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "#     # Data Augmentation\n",
        "#     datagen = ImageDataGenerator(\n",
        "#         rotation_range=15,\n",
        "#         width_shift_range=0.1,\n",
        "#         height_shift_range=0.1,\n",
        "#         horizontal_flip=True\n",
        "#     )\n",
        "\n",
        "#     datagen.fit(X_train)\n",
        "\n",
        "#     # Convert labels to single-dimensional integers\n",
        "#     y_train = y_train.flatten()\n",
        "#     y_test = y_test.flatten()\n",
        "\n",
        "#     # Split training data into training and validation sets\n",
        "#     train_size = int(0.8 * len(X_train))\n",
        "#     X_train, X_valid = X_train[:train_size], X_train[train_size:]\n",
        "#     y_train, y_valid = y_train[:train_size], y_train[train_size:]\n",
        "\n",
        "#     # Load the CNN model and extract features\n",
        "#     cnn_model = load_cnn_model()\n",
        "\n",
        "#     X_train_cnn = get_feature_layer(cnn_model, X_train)\n",
        "#     X_valid_cnn = get_feature_layer(cnn_model, X_valid)\n",
        "#     X_test_cnn = get_feature_layer(cnn_model, X_test)\n",
        "\n",
        "#     # Combine training and validation data\n",
        "#     X_combined_cnn = np.vstack((X_train_cnn, X_valid_cnn))\n",
        "#     y_combined = np.hstack((y_train, y_valid))\n",
        "\n",
        "#     # Define the parameter grid\n",
        "#     param_grid = {\n",
        "#         'learning_rate': [0.01, 0.1, 0.2],\n",
        "#         'max_depth': [6, 10, 15],\n",
        "#         'min_child_weight': [1, 5, 10],\n",
        "#         'subsample': [0.8, 0.9, 1.0],\n",
        "#         'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "#         'n_estimators': [100, 300, 500]\n",
        "#     }\n",
        "\n",
        "#     # Create DMatrix for combined training data\n",
        "#     dtrain_combined = xgb.DMatrix(X_combined_cnn, label=y_combined)\n",
        "\n",
        "#     # Initialize XGBoost model\n",
        "#     xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=10)\n",
        "\n",
        "#     # Perform grid search\n",
        "#     grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1, n_jobs=-1)\n",
        "#     grid_search.fit(X_combined_cnn, y_combined)\n",
        "\n",
        "#     # Get the best parameters and best model\n",
        "#     best_params = grid_search.best_params_\n",
        "#     print(\"Best hyperparameters found were:\", best_params)\n",
        "\n",
        "#     # Train the final model with the best hyperparameters\n",
        "#     final_model = xgb.XGBClassifier(**best_params, objective='multi:softprob', num_class=10)\n",
        "#     final_model.fit(X_combined_cnn, y_combined)\n",
        "\n",
        "#     # Save the final model\n",
        "#     pickle.dump(final_model, open('optimized_xgboost_model_gridsearch.pkl', 'wb'))\n",
        "\n",
        "#     # Evaluate the final model\n",
        "#     evaluate_model(final_model.get_booster(), X_test_cnn, y_test)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChalvlS51Ylf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# 2. Preprocess data: normalize images and flatten labels\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Flatten labels\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "\n",
        "# 3. Define a function to extract features using a simple CNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "def create_feature_extractor():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train a simple CNN to extract features\n",
        "feature_extractor = create_feature_extractor()\n",
        "feature_extractor.fit(X_train, to_categorical(y_train), epochs=10, batch_size=64, validation_split=0.2, verbose=2)\n",
        "\n",
        "# Remove the last layer to get the feature extractor\n",
        "feature_extractor = Sequential(feature_extractor.layers[:-1])\n",
        "\n",
        "# 4. Extract features from the training and test data\n",
        "X_train_features = feature_extractor.predict(X_train)\n",
        "X_test_features = feature_extractor.predict(X_test)\n",
        "\n",
        "# 5. Train XGBoost model on extracted features\n",
        "dtrain = xgb.DMatrix(X_train_features, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test_features, label=y_test)\n",
        "\n",
        "params = {\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': 10,\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100\n",
        "}\n",
        "\n",
        "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# 6. Predict and evaluate the model\n",
        "y_pred = bst.predict(dtest)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_labels)\n",
        "precision = precision_score(y_test, y_pred_labels, average='weighted')\n",
        "recall = recall_score(y_test, y_pred_labels, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred_labels, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DIPAKAI"
      ],
      "metadata": {
        "id": "ZckQ-S0dwKfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define paths to dataset\n",
        "train_dir = '/content/drive/MyDrive/intel/seg_train/seg_train'\n",
        "test_dir = '/content/drive/MyDrive/intel/seg_test/seg_test'\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load data\n",
        "train_generator = datagen.flow_from_directory(train_dir, target_size=(150, 150), batch_size=256, class_mode='categorical')\n",
        "test_generator = datagen.flow_from_directory(test_dir, target_size=(150, 150), batch_size=256, class_mode='categorical')\n",
        "\n",
        "# Define class names\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "num_classes = len(class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQi460LmwMCs",
        "outputId": "9def0352-f623-441c-8b8e-f4ec46cc7be9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14044 images belonging to 6 classes.\n",
            "Found 3000 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the CNN model\n",
        "model.fit(train_generator, epochs=5, verbose=1, validation_data=test_generator)\n",
        "\n",
        "# Save the CNN model\n",
        "model.save('cnn_intel_image.h5')\n",
        "\n",
        "# Evaluate CNN model\n",
        "test_images, test_labels = next(test_generator)\n",
        "cnn_preds = model.predict(test_images)\n",
        "cnn_preds_classes = np.argmax(cnn_preds, axis=1)\n",
        "true_classes = np.argmax(test_labels, axis=1)\n",
        "cnn_accuracy = accuracy_score(true_classes, cnn_preds_classes)\n",
        "cnn_precision = precision_score(true_classes, cnn_preds_classes, average='weighted')\n",
        "cnn_recall = recall_score(true_classes, cnn_preds_classes, average='weighted')\n",
        "cnn_f1 = f1_score(true_classes, cnn_preds_classes, average='weighted')\n",
        "print(f'CNN Accuracy: {cnn_accuracy}')\n",
        "print(f'CNN Precision: {cnn_precision}')\n",
        "print(f'CNN Recall: {cnn_recall}')\n",
        "print(f'CNN F1-Score: {cnn_f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwIYyUlFwPoU",
        "outputId": "93e1865d-ad37-4b96-f115-94159c8e8d8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "55/55 [==============================] - 84s 1s/step - loss: 1.2607 - accuracy: 0.5140 - val_loss: 0.9450 - val_accuracy: 0.6443\n",
            "Epoch 2/5\n",
            "55/55 [==============================] - 74s 1s/step - loss: 0.8266 - accuracy: 0.6888 - val_loss: 0.7577 - val_accuracy: 0.7213\n",
            "Epoch 3/5\n",
            "55/55 [==============================] - 78s 1s/step - loss: 0.6852 - accuracy: 0.7503 - val_loss: 0.6304 - val_accuracy: 0.7760\n",
            "Epoch 4/5\n",
            "55/55 [==============================] - 72s 1s/step - loss: 0.5779 - accuracy: 0.7925 - val_loss: 0.5460 - val_accuracy: 0.8017\n",
            "Epoch 5/5\n",
            "55/55 [==============================] - 74s 1s/step - loss: 0.5011 - accuracy: 0.8238 - val_loss: 0.6055 - val_accuracy: 0.7793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 11ms/step\n",
            "CNN Accuracy: 0.7265625\n",
            "CNN Precision: 0.765094429560388\n",
            "CNN Recall: 0.7265625\n",
            "CNN F1-Score: 0.7287589059443118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CNN model\n",
        "model = load_model('cnn_intel_image.h5')\n",
        "\n",
        "# Create a feature extractor model\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(generator, model):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = model.predict(inputs_batch)\n",
        "        features.append(features_batch)\n",
        "        labels.append(labels_batch)\n",
        "        if len(features) * generator.batch_size >= generator.samples:\n",
        "            break\n",
        "    return np.concatenate(features), np.concatenate(labels)\n",
        "\n",
        "# Extract features for training and test sets\n",
        "train_features, train_labels = extract_features(train_generator, feature_extractor)\n",
        "test_features, test_labels = extract_features(test_generator, feature_extractor)\n",
        "\n",
        "# Convert labels to one-dimensional array\n",
        "train_labels_flat = np.argmax(train_labels, axis=1)\n",
        "test_labels_flat = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Define XGBoost model parameters (without TPE)\n",
        "xgb_params = {\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': num_classes,\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "}\n",
        "\n",
        "# Train the XGBoost model without TPE\n",
        "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "xgb_model.fit(train_features, train_labels_flat)\n",
        "\n",
        "# Evaluate XGBoost model without TPE\n",
        "xgb_preds = xgb_model.predict(test_features)\n",
        "xgb_accuracy = accuracy_score(test_labels_flat, xgb_preds)\n",
        "xgb_precision = precision_score(test_labels_flat, xgb_preds, average='weighted')\n",
        "xgb_recall = recall_score(test_labels_flat, xgb_preds, average='weighted')\n",
        "xgb_f1 = f1_score(test_labels_flat, xgb_preds, average='weighted')\n",
        "print(f'XGBoost Accuracy: {xgb_accuracy}')\n",
        "print(f'XGBoost Precision: {xgb_precision}')\n",
        "print(f'XGBoost Recall: {xgb_recall}')\n",
        "print(f'XGBoost F1-Score: {xgb_f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-eEQ_kewVu-",
        "outputId": "2d6d8981-675c-485f-d5e6-1e17dca46650"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 11ms/step\n",
            "8/8 [==============================] - 0s 10ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "7/7 [==============================] - 0s 65ms/step\n",
            "8/8 [==============================] - 0s 6ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "6/6 [==============================] - 0s 73ms/step\n",
            "8/8 [==============================] - 0s 6ms/step\n",
            "XGBoost Accuracy: 0.83\n",
            "XGBoost Precision: 0.8298612077136445\n",
            "XGBoost Recall: 0.83\n",
            "XGBoost F1-Score: 0.8298795477915067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CNN model\n",
        "model = load_model('cnn_intel_image.h5')\n",
        "\n",
        "# Create a feature extractor model\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "# Extract features for training and test sets\n",
        "train_features, train_labels = extract_features(train_generator, feature_extractor)\n",
        "test_features, test_labels = extract_features(test_generator, feature_extractor)\n",
        "\n",
        "# Convert labels to one-dimensional array\n",
        "train_labels_flat = np.argmax(train_labels, axis=1)\n",
        "test_labels_flat = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Define objective function for Optuna (with TPE)\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'multi:softmax',\n",
        "        'num_class': num_classes,\n",
        "        'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 15),\n",
        "        'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 5),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1),\n",
        "        'base_score': trial.suggest_uniform('base_score', 0.1, 0.9),\n",
        "        'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.1, 1),\n",
        "        'colsample_bynode': trial.suggest_uniform('colsample_bynode', 0.1, 1),\n",
        "        'max_delta_step': trial.suggest_uniform('max_delta_step', 0, 1),\n",
        "        'num_parallel_tree': trial.suggest_int('num_parallel_tree', 1, 10),\n",
        "        'gamma': trial.suggest_uniform('gamma', 0, 0.1),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**param)\n",
        "    model.fit(train_features, train_labels_flat)\n",
        "\n",
        "    preds = model.predict(test_features)\n",
        "    accuracy = accuracy_score(test_labels_flat, preds)\n",
        "    return accuracy\n",
        "\n",
        "# Create study and optimize\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective, n_trials=5)\n",
        "\n",
        "# Train the final model with the best parameters (with TPE)\n",
        "best_params = study.best_params\n",
        "final_model = xgb.XGBClassifier(**best_params)\n",
        "final_model.fit(train_features, train_labels_flat)\n",
        "\n",
        "# Evaluate the model with TPE\n",
        "tpe_preds = final_model.predict(test_features)\n",
        "tpe_accuracy = accuracy_score(test_labels_flat, tpe_preds)\n",
        "tpe_precision = precision_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "tpe_recall = recall_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "tpe_f1 = f1_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "print(f'XGBoost with TPE Accuracy: {tpe_accuracy}')\n",
        "print(f'XGBoost with TPE Precision: {tpe_precision}')\n",
        "print(f'XGBoost with TPE Recall: {tpe_recall}')\n",
        "print(f'XGBoost with TPE F1-Score: {tpe_f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdSuIMTxwYZH",
        "outputId": "c2d637e2-fa4e-4a76-80d8-4ff2b455072a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "7/7 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "6/6 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-08 08:17:58,617] A new study created in memory with name: no-name-a8aaf9ba-cf6b-4989-9aa6-189f793c216b\n",
            "<ipython-input-19-a5c6f79c9aa4>:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 5),\n",
            "<ipython-input-19-a5c6f79c9aa4>:23: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:24: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:25: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'base_score': trial.suggest_uniform('base_score', 0.1, 0.9),\n",
            "<ipython-input-19-a5c6f79c9aa4>:26: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:27: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bynode': trial.suggest_uniform('colsample_bynode', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:28: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'max_delta_step': trial.suggest_uniform('max_delta_step', 0, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.1),\n",
            "[I 2024-08-08 08:20:53,968] Trial 0 finished with value: 0.8133333333333334 and parameters: {'learning_rate': 0.018586041149256504, 'max_depth': 5, 'min_child_weight': 2.536041896884144, 'subsample': 0.562673158757751, 'colsample_bytree': 0.9763368965810157, 'base_score': 0.2503328035922161, 'colsample_bylevel': 0.2158750350472744, 'colsample_bynode': 0.9137135031558699, 'max_delta_step': 0.37129111223144506, 'num_parallel_tree': 5, 'gamma': 0.08163899401465244, 'n_estimators': 168}. Best is trial 0 with value: 0.8133333333333334.\n",
            "<ipython-input-19-a5c6f79c9aa4>:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 5),\n",
            "<ipython-input-19-a5c6f79c9aa4>:23: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:24: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:25: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'base_score': trial.suggest_uniform('base_score', 0.1, 0.9),\n",
            "<ipython-input-19-a5c6f79c9aa4>:26: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:27: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bynode': trial.suggest_uniform('colsample_bynode', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:28: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'max_delta_step': trial.suggest_uniform('max_delta_step', 0, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.1),\n",
            "[I 2024-08-08 08:42:23,755] Trial 1 finished with value: 0.8193333333333334 and parameters: {'learning_rate': 0.003557773946487262, 'max_depth': 14, 'min_child_weight': 0.5368869051436415, 'subsample': 0.7180307799278278, 'colsample_bytree': 0.7398255535101577, 'base_score': 0.5688472635459227, 'colsample_bylevel': 0.7261341315493617, 'colsample_bynode': 0.538654233903584, 'max_delta_step': 0.8268724944128614, 'num_parallel_tree': 4, 'gamma': 0.031683835254944184, 'n_estimators': 300}. Best is trial 1 with value: 0.8193333333333334.\n",
            "<ipython-input-19-a5c6f79c9aa4>:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 5),\n",
            "<ipython-input-19-a5c6f79c9aa4>:23: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:24: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:25: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'base_score': trial.suggest_uniform('base_score', 0.1, 0.9),\n",
            "<ipython-input-19-a5c6f79c9aa4>:26: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:27: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bynode': trial.suggest_uniform('colsample_bynode', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:28: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'max_delta_step': trial.suggest_uniform('max_delta_step', 0, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.1),\n",
            "[I 2024-08-08 08:44:28,969] Trial 2 finished with value: 0.8106666666666666 and parameters: {'learning_rate': 0.01717142739911604, 'max_depth': 10, 'min_child_weight': 3.400105735148351, 'subsample': 0.2228019937786932, 'colsample_bytree': 0.11784748093815751, 'base_score': 0.8309998817391637, 'colsample_bylevel': 0.7805695216894306, 'colsample_bynode': 0.47868412043362163, 'max_delta_step': 0.29434944876690794, 'num_parallel_tree': 1, 'gamma': 0.03316563762525653, 'n_estimators': 510}. Best is trial 1 with value: 0.8193333333333334.\n",
            "<ipython-input-19-a5c6f79c9aa4>:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 5),\n",
            "<ipython-input-19-a5c6f79c9aa4>:23: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:24: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:25: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'base_score': trial.suggest_uniform('base_score', 0.1, 0.9),\n",
            "<ipython-input-19-a5c6f79c9aa4>:26: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:27: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bynode': trial.suggest_uniform('colsample_bynode', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:28: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'max_delta_step': trial.suggest_uniform('max_delta_step', 0, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.1),\n",
            "[I 2024-08-08 09:03:46,569] Trial 3 finished with value: 0.8213333333333334 and parameters: {'learning_rate': 0.03051311746330259, 'max_depth': 7, 'min_child_weight': 0.25242228341085804, 'subsample': 0.8616128440105155, 'colsample_bytree': 0.21284959905285222, 'base_score': 0.1769874138426505, 'colsample_bylevel': 0.9754825095591855, 'colsample_bynode': 0.8619869281114191, 'max_delta_step': 0.19803764060737472, 'num_parallel_tree': 6, 'gamma': 0.08156767532188891, 'n_estimators': 519}. Best is trial 3 with value: 0.8213333333333334.\n",
            "<ipython-input-19-a5c6f79c9aa4>:20: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:22: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 0, 5),\n",
            "<ipython-input-19-a5c6f79c9aa4>:23: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:24: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:25: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'base_score': trial.suggest_uniform('base_score', 0.1, 0.9),\n",
            "<ipython-input-19-a5c6f79c9aa4>:26: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:27: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bynode': trial.suggest_uniform('colsample_bynode', 0.1, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:28: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'max_delta_step': trial.suggest_uniform('max_delta_step', 0, 1),\n",
            "<ipython-input-19-a5c6f79c9aa4>:30: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.1),\n",
            "[I 2024-08-08 09:11:08,104] Trial 4 finished with value: 0.8236666666666667 and parameters: {'learning_rate': 0.0667368218914316, 'max_depth': 9, 'min_child_weight': 2.466921752417947, 'subsample': 0.7304477822119838, 'colsample_bytree': 0.33075689723318014, 'base_score': 0.7872659177911435, 'colsample_bylevel': 0.5049455853113113, 'colsample_bynode': 0.3511141632769116, 'max_delta_step': 0.6358667902879936, 'num_parallel_tree': 8, 'gamma': 0.0035912814455600576, 'n_estimators': 127}. Best is trial 4 with value: 0.8236666666666667.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with TPE Accuracy: 0.8236666666666667\n",
            "XGBoost with TPE Precision: 0.8239354174586377\n",
            "XGBoost with TPE Recall: 0.8236666666666667\n",
            "XGBoost with TPE F1-Score: 0.8236713372549874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model, load_model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "# Fungsi untuk mengekstraksi fitur\n",
        "def extract_features(generator, model):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for i in range(len(generator)):\n",
        "        x, y = generator[i]\n",
        "        feature = model.predict(x)\n",
        "        features.append(feature)\n",
        "        labels.append(y)\n",
        "    return np.concatenate(features), np.concatenate(labels)\n",
        "\n",
        "# Load the CNN model\n",
        "model = load_model('cnn_intel_image.h5')\n",
        "\n",
        "# Create a feature extractor model\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-3].output)\n",
        "\n",
        "# Extract features for training and test sets\n",
        "train_features, train_labels = extract_features(train_generator, feature_extractor)\n",
        "test_features, test_labels = extract_features(test_generator, feature_extractor)\n",
        "\n",
        "# Convert labels to one-dimensional array\n",
        "train_labels_flat = np.argmax(train_labels, axis=1)\n",
        "test_labels_flat = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Define objective function for Optuna (with TPE)\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'multi:softmax',\n",
        "        'num_class': len(np.unique(train_labels_flat)),\n",
        "        'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'min_child_weight': trial.suggest_uniform('min_child_weight', 1, 10),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
        "        'gamma': trial.suggest_uniform('gamma', 0, 0.5),\n",
        "        'alpha': trial.suggest_uniform('alpha', 0, 0.5),\n",
        "        'lambda': trial.suggest_uniform('lambda', 0, 0.5),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBClassifier(**param, use_label_encoder=False, eval_metric='mlogloss')\n",
        "    model.fit(train_features, train_labels_flat)\n",
        "\n",
        "    preds = model.predict(test_features)\n",
        "    accuracy = accuracy_score(test_labels_flat, preds)\n",
        "    return accuracy\n",
        "\n",
        "# Create study and optimize\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective, n_trials=5)  # Meningkatkan jumlah trials\n",
        "\n",
        "# Train the final model with the best parameters (with TPE)\n",
        "best_params = study.best_params\n",
        "final_model = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='mlogloss')\n",
        "final_model.fit(train_features, train_labels_flat)\n",
        "\n",
        "# Evaluate the model with TPE\n",
        "tpe_preds = final_model.predict(test_features)\n",
        "tpe_accuracy = accuracy_score(test_labels_flat, tpe_preds)\n",
        "tpe_precision = precision_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "tpe_recall = recall_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "tpe_f1 = f1_score(test_labels_flat, tpe_preds, average='weighted')\n",
        "print(f'XGBoost with TPE Accuracy: {tpe_accuracy}')\n",
        "print(f'XGBoost with TPE Precision: {tpe_precision}')\n",
        "print(f'XGBoost with TPE Recall: {tpe_recall}')\n",
        "print(f'XGBoost with TPE F1-Score: {tpe_f1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66sJyVqUSNfx",
        "outputId": "1431df54-489a-4d03-d242-3c1281a8204b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "7/7 [==============================] - 0s 10ms/step\n",
            "8/8 [==============================] - 0s 12ms/step\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "8/8 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 8ms/step\n",
            "6/6 [==============================] - 0s 8ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-08-08 09:22:23,961] A new study created in memory with name: no-name-361212fd-8582-4d8e-a5ad-903dca0f4401\n",
            "<ipython-input-20-1ad406fad8c1>:37: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-20-1ad406fad8c1>:39: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 1, 10),\n",
            "<ipython-input-20-1ad406fad8c1>:40: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
            "<ipython-input-20-1ad406fad8c1>:41: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
            "<ipython-input-20-1ad406fad8c1>:42: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.5),\n",
            "<ipython-input-20-1ad406fad8c1>:43: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'alpha': trial.suggest_uniform('alpha', 0, 0.5),\n",
            "<ipython-input-20-1ad406fad8c1>:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'lambda': trial.suggest_uniform('lambda', 0, 0.5),\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [09:22:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2024-08-08 09:25:16,915] Trial 0 finished with value: 0.8333333333333334 and parameters: {'learning_rate': 0.07576056937941027, 'max_depth': 15, 'min_child_weight': 1.9333992731477143, 'subsample': 0.6536973401239022, 'colsample_bytree': 0.9986443497432869, 'gamma': 0.40531971954731155, 'alpha': 0.027056558065263192, 'lambda': 0.23151456884036958, 'n_estimators': 666}. Best is trial 0 with value: 0.8333333333333334.\n",
            "<ipython-input-20-1ad406fad8c1>:37: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-20-1ad406fad8c1>:39: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 1, 10),\n",
            "<ipython-input-20-1ad406fad8c1>:40: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
            "<ipython-input-20-1ad406fad8c1>:41: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
            "<ipython-input-20-1ad406fad8c1>:42: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.5),\n",
            "<ipython-input-20-1ad406fad8c1>:43: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'alpha': trial.suggest_uniform('alpha', 0, 0.5),\n",
            "<ipython-input-20-1ad406fad8c1>:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'lambda': trial.suggest_uniform('lambda', 0, 0.5),\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [09:25:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2024-08-08 09:27:32,694] Trial 1 finished with value: 0.826 and parameters: {'learning_rate': 0.08694374759122395, 'max_depth': 6, 'min_child_weight': 2.6951900658029655, 'subsample': 0.8190692587906576, 'colsample_bytree': 0.5847167567141833, 'gamma': 0.03287763961901774, 'alpha': 0.3349503709944948, 'lambda': 0.31176460246069004, 'n_estimators': 434}. Best is trial 0 with value: 0.8333333333333334.\n",
            "<ipython-input-20-1ad406fad8c1>:37: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-20-1ad406fad8c1>:39: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 1, 10),\n",
            "<ipython-input-20-1ad406fad8c1>:40: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
            "<ipython-input-20-1ad406fad8c1>:41: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
            "<ipython-input-20-1ad406fad8c1>:42: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.5),\n",
            "<ipython-input-20-1ad406fad8c1>:43: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'alpha': trial.suggest_uniform('alpha', 0, 0.5),\n",
            "<ipython-input-20-1ad406fad8c1>:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'lambda': trial.suggest_uniform('lambda', 0, 0.5),\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [09:27:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2024-08-08 09:31:38,126] Trial 2 finished with value: 0.8323333333333334 and parameters: {'learning_rate': 0.042073659403213115, 'max_depth': 11, 'min_child_weight': 1.2085100135859683, 'subsample': 0.5911735353360312, 'colsample_bytree': 0.6123116562022413, 'gamma': 0.2850294578425941, 'alpha': 0.028556986369771664, 'lambda': 0.3641145665383867, 'n_estimators': 756}. Best is trial 0 with value: 0.8333333333333334.\n",
            "<ipython-input-20-1ad406fad8c1>:37: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-20-1ad406fad8c1>:39: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 1, 10),\n",
            "<ipython-input-20-1ad406fad8c1>:40: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
            "<ipython-input-20-1ad406fad8c1>:41: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
            "<ipython-input-20-1ad406fad8c1>:42: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.5),\n",
            "<ipython-input-20-1ad406fad8c1>:43: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'alpha': trial.suggest_uniform('alpha', 0, 0.5),\n",
            "<ipython-input-20-1ad406fad8c1>:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'lambda': trial.suggest_uniform('lambda', 0, 0.5),\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [09:31:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2024-08-08 09:33:24,315] Trial 3 finished with value: 0.8293333333333334 and parameters: {'learning_rate': 0.08826061463242345, 'max_depth': 14, 'min_child_weight': 6.862056520294111, 'subsample': 0.8146118977542989, 'colsample_bytree': 0.6892851138628163, 'gamma': 0.3464689002316005, 'alpha': 0.18156698974385477, 'lambda': 0.16534893490305153, 'n_estimators': 429}. Best is trial 0 with value: 0.8333333333333334.\n",
            "<ipython-input-20-1ad406fad8c1>:37: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
            "<ipython-input-20-1ad406fad8c1>:39: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'min_child_weight': trial.suggest_uniform('min_child_weight', 1, 10),\n",
            "<ipython-input-20-1ad406fad8c1>:40: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.5, 1),\n",
            "<ipython-input-20-1ad406fad8c1>:41: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1),\n",
            "<ipython-input-20-1ad406fad8c1>:42: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'gamma': trial.suggest_uniform('gamma', 0, 0.5),\n",
            "<ipython-input-20-1ad406fad8c1>:43: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'alpha': trial.suggest_uniform('alpha', 0, 0.5),\n",
            "<ipython-input-20-1ad406fad8c1>:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'lambda': trial.suggest_uniform('lambda', 0, 0.5),\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [09:33:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2024-08-08 09:34:35,237] Trial 4 finished with value: 0.829 and parameters: {'learning_rate': 0.0946924568109594, 'max_depth': 6, 'min_child_weight': 5.258307213274756, 'subsample': 0.6488307800555189, 'colsample_bytree': 0.8911563365869846, 'gamma': 0.3769059815483292, 'alpha': 0.19632154324021078, 'lambda': 0.4430791345582778, 'n_estimators': 211}. Best is trial 0 with value: 0.8333333333333334.\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [09:34:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost with TPE Accuracy: 0.8333333333333334\n",
            "XGBoost with TPE Precision: 0.8333387299610068\n",
            "XGBoost with TPE Recall: 0.8333333333333334\n",
            "XGBoost with TPE F1-Score: 0.8332496637738485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Path ke dataset\n",
        "dataset_path_train = '/content/drive/MyDrive/intel/seg_train/'\n",
        "dataset_path_test = '/content/drive/MyDrive/intel/seg_test/'\n",
        "\n",
        "# Kelas yang ada di dataset\n",
        "classes = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n",
        "\n",
        "# Fungsi untuk membaca dan preprocess gambar\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for class_idx, class_name in enumerate(classes):\n",
        "        class_folder = os.path.join(folder, class_name)\n",
        "        for filename in os.listdir(class_folder):\n",
        "            img_path = os.path.join(class_folder, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, (64, 64))  # Resize gambar ke ukuran yang sama\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Ubah ke grayscale\n",
        "                images.append(img)\n",
        "                labels.append(class_idx)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load data train dan test\n",
        "train_images, train_labels = load_images_from_folder(os.path.join(dataset_path_train, 'seg_train'))\n",
        "test_images, test_labels = load_images_from_folder(os.path.join(dataset_path_test, 'seg_test'))\n",
        "\n",
        "# Flatten gambar untuk ekstraksi fitur\n",
        "train_images_flatten = train_images.reshape(train_images.shape[0], -1)\n",
        "test_images_flatten = test_images.reshape(test_images.shape[0], -1)\n",
        "\n",
        "# Split data train menjadi train dan validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_images_flatten, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Buat dan latih model XGBoost\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi pada data validation\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_precision = precision_score(y_val, y_val_pred, average='weighted')\n",
        "val_recall = recall_score(y_val, y_val_pred, average='weighted')\n",
        "val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
        "\n",
        "print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n",
        "print(f'Validation Precision: {val_precision * 100:.2f}%')\n",
        "print(f'Validation Recall: {val_recall * 100:.2f}%')\n",
        "print(f'Validation F1-Score: {val_f1 * 100:.2f}%')\n",
        "\n",
        "# Prediksi pada data test\n",
        "y_test_pred = model.predict(test_images_flatten)\n",
        "test_accuracy = accuracy_score(test_labels, y_test_pred)\n",
        "test_precision = precision_score(test_labels, y_test_pred, average='weighted')\n",
        "test_recall = recall_score(test_labels, y_test_pred, average='weighted')\n",
        "test_f1 = f1_score(test_labels, y_test_pred, average='weighted')\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Test Precision: {test_precision * 100:.2f}%')\n",
        "print(f'Test Recall: {test_recall * 100:.2f}%')\n",
        "print(f'Test F1-Score: {test_f1 * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfUtmkRqdVef",
        "outputId": "2bdc6ca6-f118-44f6-ef80-57316b90844d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 58.81%\n",
            "Validation Precision: 58.66%\n",
            "Validation Recall: 58.81%\n",
            "Validation F1-Score: 58.33%\n",
            "Test Accuracy: 57.93%\n",
            "Test Precision: 57.85%\n",
            "Test Recall: 57.93%\n",
            "Test F1-Score: 57.56%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}